<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>mactql的小站</title>
  
  
  <link href="https://mactql.github.io/atom.xml" rel="self"/>
  
  <link href="https://mactql.github.io/"/>
  <updated>2021-10-15T12:15:30.974Z</updated>
  <id>https://mactql.github.io/</id>
  
  <author>
    <name>mactql</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《NAS-BERT：神经架构搜索与自适应BERT压缩》论文笔记</title>
    <link href="https://mactql.github.io/posts/2369829638.html"/>
    <id>https://mactql.github.io/posts/2369829638.html</id>
    <published>2021-10-15T12:09:00.000Z</published>
    <updated>2021-10-15T12:15:30.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><p><strong>题目：《NAS-BERT: Task-agnostic and Adaptive-size BERT Compression with Neural Architecture Search》</strong></p><hr><h2 id="理论方法阐释"><a href="#理论方法阐释" class="headerlink" title="理论方法阐释"></a>理论方法阐释</h2><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8ANAS-BERT%EF%BC%9A%E7%A5%9E%E7%BB%8F%E6%9E%B6%E6%9E%84%E6%90%9C%E7%B4%A2%E4%B8%8E%E8%87%AA%E9%80%82%E5%BA%94BERT%E5%8E%8B%E7%BC%A9%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><p><strong>首先给定一个Teacher的模型，在论文中用的是BERT-base。把这个Teacher模型均匀的划分为四个Block，例如前三层作为一个Block，后面以此类推。对于每一个Blcok，我们建立一个NAS Search Block去学习/模仿对应Block的信息或者说学习能力</strong><br><strong>具体的做法是这样的，一个数据流进来，通过对数据流forward可以得到对应数据流的引表针，那么对每一个Block都可以得到他们的输入输出，每个Block的输入输出代表了他们的函数映射。为了模仿这个函数映射，我们就用这个映射去训练一个NAS Search Block来学习对应的Block的能力，NAS Search Block就是图(a)中所示的框架</strong><br><strong>NAS Search Block假设有六层，每一层有c个 candidate Operation，通过在每一层选择 candidate Operation构成一条自底向上的路径，可以看到可以有非常多的路径可以选择，通过训练得到一条性能最好的路径得到架构</strong><br><strong>candidate Operation有以下几种选择</strong></p><ul><li><strong>Multi-head Attention</strong></li><li><strong>Feed-forward Network</strong></li><li><strong>卷积</strong></li><li><strong>无Operation</strong></li></ul><hr><h2 id="实验方法阐释"><a href="#实验方法阐释" class="headerlink" title="实验方法阐释"></a>实验方法阐释</h2><ul><li><strong>在实验中使用的Teacher模型，是一个100M的标准的BERT base的模型，训练中也是使用标准的16GB的数据</strong></li><li><strong>模型使用的是12层的Transformer，hidden size 768</strong></li><li><strong>super-net用的是24层，为什么用24层呢，是因为一个Transformer里有两层，Multi-head Attention和Feed-forward Network</strong></li><li><strong>为了评估架构的模型效果，对架构进行重新训练。由于重训的代价比较高，论文中采用了5，10，30，60M四种模型的setting</strong><ul><li><strong>NAS-BERT-5, NAS-BERT-10, NAS-BERT-30, NAS-BERTE-60</strong></li><li><strong>在八个GLUE 的benchmark上以及SquAD1.0和SquAD2.0数据上做了性能的评测</strong></li></ul></li></ul><h2 id="实验1"><a href="#实验1" class="headerlink" title="实验1"></a>实验1</h2><p><strong>首先在5，10，30，60M四种模型尺寸的setting下和不同训练方式下对比NAS-BERT和BERT的效果</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8ANAS-BERT%EF%BC%9A%E7%A5%9E%E7%BB%8F%E6%9E%B6%E6%9E%84%E6%90%9C%E7%B4%A2%E4%B8%8E%E8%87%AA%E9%80%82%E5%BA%94BERT%E5%8E%8B%E7%BC%A9%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>PF和KD是在两种不同的训练方式去对比模型的效果</strong><ul><li><strong>PF表示pre-training和fine-training两个基本的范式</strong></li><li><strong>KD表示用压缩的方式做两阶段的知识蒸馏</strong></li></ul></li><li><strong>测试了模型的计算量(FLOPs)、加速比(Speedup)、八个数据集以及八个数据集的平均值</strong></li><li><strong>可以看到在不同的模型Setting下，NAS-BERT都显著超过BERT模型，并且当模型越小差异值越大</strong></li></ul><h2 id="实验2"><a href="#实验2" class="headerlink" title="实验2"></a>实验2</h2><p><strong>和之前的模型压缩的工作进行对比</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8ANAS-BERT%EF%BC%9A%E7%A5%9E%E7%BB%8F%E6%9E%B6%E6%9E%84%E6%90%9C%E7%B4%A2%E4%B8%8E%E8%87%AA%E9%80%82%E5%BA%94BERT%E5%8E%8B%E7%BC%A9%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8ANAS-BERT%EF%BC%9A%E7%A5%9E%E7%BB%8F%E6%9E%B6%E6%9E%84%E6%90%9C%E7%B4%A2%E4%B8%8E%E8%87%AA%E9%80%82%E5%BA%94BERT%E5%8E%8B%E7%BC%A9%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/3.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>在八种数据集上以及SQuAD1.1和2.0数据集上分别在验证集和测试集上进行对比，还加入了数据增广的模型进行对比，如图中带*的模型</strong></li><li><strong>可以看到不管是在验证集还是测试集上，NAS-BERT都要显著超过之前的方法</strong></li></ul><h2 id="实验3"><a href="#实验3" class="headerlink" title="实验3"></a>实验3</h2><p><strong>测试如果提出的NAS-BERT不使用progressive shrinking的性能效果</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8ANAS-BERT%EF%BC%9A%E7%A5%9E%E7%BB%8F%E6%9E%B6%E6%9E%84%E6%90%9C%E7%B4%A2%E4%B8%8E%E8%87%AA%E9%80%82%E5%BA%94BERT%E5%8E%8B%E7%BC%A9%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/4.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"><br><strong>在第4个epoch的时候加入progressive shrinking，即图中红线所示。蓝线是不加入progressive shrinking</strong><br><strong>可以看到，加入progressive shrinking以后，loss是快速下降的，也就证明super-net的收敛速度是显著加快的</strong><br><strong>从表格上也可以看出，使用了progressive shrinking是显著超过不使用progressive shrinking的</strong></p><h2 id="实验4"><a href="#实验4" class="headerlink" title="实验4"></a>实验4</h2><p><strong>测试不同的shrinking方法的性能效果</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8ANAS-BERT%EF%BC%9A%E7%A5%9E%E7%BB%8F%E6%9E%B6%E6%9E%84%E6%90%9C%E7%B4%A2%E4%B8%8E%E8%87%AA%E9%80%82%E5%BA%94BERT%E5%8E%8B%E7%BC%A9%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/5.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><h2 id="实验5"><a href="#实验5" class="headerlink" title="实验5"></a>实验5</h2><p><strong>对比测试在不同的蒸馏setting下的NAS-BERT和BERT的性能效果</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8ANAS-BERT%EF%BC%9A%E7%A5%9E%E7%BB%8F%E6%9E%B6%E6%9E%84%E6%90%9C%E7%B4%A2%E4%B8%8E%E8%87%AA%E9%80%82%E5%BA%94BERT%E5%8E%8B%E7%BC%A9%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/6.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"><br><strong>实验分别测试了仅上游使用蒸馏、仅下游使用蒸馏的以及上下游同时使用蒸馏</strong><br><strong>在八个数据集上NAS-BERT都显著超过BERT模型，证明了NAS-BERT模型并不是只有在两阶段蒸馏的情况下才有效</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;&lt;a href=&quot;#基本信息&quot; class=&quot;headerlink&quot; title=&quot;基本信息&quot;&gt;&lt;/a&gt;基本信息&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;题目：《NAS-BERT: Task-agnostic and Adaptive-size BERT Com</summary>
      
    
    
    
    <category term="硕士每周论文笔记" scheme="https://mactql.github.io/categories/%E7%A1%95%E5%A3%AB%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Transformer" scheme="https://mactql.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Spark job中的stage划分与三种提交模式</title>
    <link href="https://mactql.github.io/posts/2922386947.html"/>
    <id>https://mactql.github.io/posts/2922386947.html</id>
    <published>2021-09-30T07:54:00.000Z</published>
    <updated>2021-09-30T08:00:55.211Z</updated>
    
    <content type="html"><![CDATA[<h2 id="首先要了解宽依赖和窄依赖是什么？"><a href="#首先要了解宽依赖和窄依赖是什么？" class="headerlink" title="首先要了解宽依赖和窄依赖是什么？"></a>首先要了解宽依赖和窄依赖是什么？</h2><ul><li><strong>窄依赖：每个RDD对应一个父RDD，每个父子RDD是一对一的关系</strong></li><li><strong>宽依赖：父RDD的partition被多个子RDD使用，父子RDD是错综复杂的关系</strong><ul><li><strong>产生了shuffle操作就是宽依赖</strong></li></ul></li></ul><hr><h2 id="什么是stage？"><a href="#什么是stage？" class="headerlink" title="什么是stage？"></a>什么是stage？</h2><p><strong>通过之前的学习，我们了解到Spark job是由action算子触发的，每个action算子触发一个job</strong><br><strong>每个job会被划分成多个stage，每个stage是由一组并行的Task来完成的</strong></p><h3 id="那么Stage是怎么划分的呢？"><a href="#那么Stage是怎么划分的呢？" class="headerlink" title="那么Stage是怎么划分的呢？"></a>那么Stage是怎么划分的呢？</h3><ul><li><strong>stage的划分依据就是看是否产生了shuffle，即是否是宽依赖</strong></li><li><strong>遇到一个shuffle(宽依赖)，就会被划分成前后两个stage，如下图所示</strong></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/Sparkjob%E4%B8%AD%E7%9A%84stage%E5%88%92%E5%88%86%E4%B8%8E%E4%B8%89%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%A8%A1%E5%BC%8F/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="下面我们学习一下Spark-job提交的三种模式"><a href="#下面我们学习一下Spark-job提交的三种模式" class="headerlink" title="下面我们学习一下Spark job提交的三种模式"></a>下面我们学习一下Spark job提交的三种模式</h2><ul><li><strong>第一种：Standalone模式</strong></li></ul><p><strong><code>spark-submit --master spark://bigdata01:7077</code></strong></p><ul><li><strong>第二种：Yarn client模式（主要用于测试）</strong></li></ul><p><strong><code>spark-submit --master yarn --deploy-mode client</code></strong></p><ul><li><strong>第三种：Yarn cluster模式（推荐）</strong></li></ul><p><strong><code>spark-submit --master yarn --deploy-mode cluster</code></strong></p><h3 id="三种模式的架构如下图所示"><a href="#三种模式的架构如下图所示" class="headerlink" title="三种模式的架构如下图所示"></a>三种模式的架构如下图所示</h3><p><img "" class="lazyload placeholder" data-original="/medias/Sparkjob%E4%B8%AD%E7%9A%84stage%E5%88%92%E5%88%86%E4%B8%8E%E4%B8%89%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%A8%A1%E5%BC%8F/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;首先要了解宽依赖和窄依赖是什么？&quot;&gt;&lt;a href=&quot;#首先要了解宽依赖和窄依赖是什么？&quot; class=&quot;headerlink&quot; title=&quot;首先要了解宽依赖和窄依赖是什么？&quot;&gt;&lt;/a&gt;首先要了解宽依赖和窄依赖是什么？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;</summary>
      
    
    
    
    <category term="《Spark性能优化的道与术》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8ASpark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>《分布式进化算法及其模型：最新进展综述》论文笔记</title>
    <link href="https://mactql.github.io/posts/824286531.html"/>
    <id>https://mactql.github.io/posts/824286531.html</id>
    <published>2021-09-30T02:31:00.000Z</published>
    <updated>2021-09-30T02:35:00.089Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><ul><li><strong>题目：Distributed evolutionary algorithms and their models: A survey of the state-of-the-art</strong></li><li><strong>作者：Gong Y-J, Chen W-N, Zhan Z-H</strong></li><li><strong>期刊：Applied Soft Computing</strong></li><li><strong>时间：2015.09</strong></li><li><strong>链接：</strong><a href="/medias/%E3%80%8A%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%A8%A1%E5%9E%8B%EF%BC%9A%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/0.png">Distributed evolutionary algorithms and their models: A survey of the state-of-the-art - ScienceDirect</a></li><li><strong>关键字：分布式进化计算；协同进化计算；进化算法；全局优化；多目标优化</strong></li></ul><hr><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li><strong>本文对最先进的分布式进化算法和模型进行了综述</strong><ul><li><strong>人口分布模型具有主从、岛、蜂窝、分层和池架构，可在人口、个体或操作级别并行化进化任务</strong></li><li><strong>维数分布模型包括协同进化和多智能体模型，它们侧重于降维</strong></li><li><strong>洞察模型，例如同步、同质性、通信、拓扑、加速、还介绍和讨论了优点和缺点</strong></li></ul></li><li><strong>还重点介绍了该领域的最新热点，包括基于云和 MapReduce 的实现、基于 GPU 和 CUDA 的实现、分布式进化多目标优化和实际应用</strong></li><li><strong>还讨论了许多未来的研究方向</strong></li></ul><hr><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><ul><li><strong>分布式进化算法框架如下图所示：</strong><ul><li><strong>基本进化算法包括：遗传算法GA，进化规划EP，进化策略ES，遗传规划GP和差分进化DE。此外蚁群算法ACO和粒子群优化PSO与进化算法具有相同特征，故也包含在内</strong></li><li><strong>分布式进化模型用来并行化处理算法，包括：常用主从/岛屿/细胞模型，另外层次/池/协同进化/多代理模型也被广泛接受</strong></li><li><strong>设计好模型后，采用不同的编程环境，包括Java、MPI、MapReduce等</strong></li><li><strong>最后用于部署的物理平台，包括集群、网格、P2P网络、云和GPU</strong></li></ul></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8A%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%A8%A1%E5%9E%8B%EF%BC%9A%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h3 id="下面介绍分布式进化模型"><a href="#下面介绍分布式进化模型" class="headerlink" title="下面介绍分布式进化模型"></a>下面介绍分布式进化模型</h3><h4 id="主从模型："><a href="#主从模型：" class="headerlink" title="主从模型："></a>主从模型：</h4><ul><li><strong>主节点进行交叉/变异/选择，把个体发给从节点进行适应度评估，从节点评估好再发送给主节点,如下图所示</strong></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8A%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%A8%A1%E5%9E%8B%EF%BC%9A%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>缺点：当适应度评估时间小于主从通信时间时，因为主从通信时间导致效率低下。</strong></li><li><strong>为了解决这个缺点，有两种主从模型的变体：</strong><ul><li><strong>从节点不仅评估适应度，还有更新任务</strong></li><li><strong>每个从节点包含一个子群，主节点从从节点接收最佳个体并把全局最佳信息发送给所有从节点</strong></li></ul></li><li><strong>总结：当适应度评估时间远大于主从通信时间时，使用主从模型效果很好</strong></li></ul><h4 id="岛屿模型："><a href="#岛屿模型：" class="headerlink" title="岛屿模型："></a>岛屿模型：</h4><ul><li><strong>把种群分成多个子种群，每个子种群对应一个处理器，个体会按照设定的时间间隔迁移到另一个岛</strong></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8A%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%A8%A1%E5%9E%8B%EF%BC%9A%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/3.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>岛屿模型节省时间，提高EA的全局搜索能力，不同岛屿中的个体可以向不同方向进化，保证种群多样性</strong></li><li><strong>缺点：通信频率低时，算法收敛速度很慢，并且迁移范围对算法性能影响很大</strong></li></ul><h4 id="细胞模型："><a href="#细胞模型：" class="headerlink" title="细胞模型："></a>细胞模型：</h4><ul><li><strong>每个个体排列在网格上，只能与相邻的个体竞争和交配</strong></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8A%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%A8%A1%E5%9E%8B%EF%BC%9A%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/4.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>可以采用并行计算来评估每个染色体的适应度，并且在局部进行交叉/变异/选择到相邻的邻居</strong></li><li><strong>优点：显著提高对所有染色体的评估速度</strong></li><li><strong>缺点：需要一个大规模的集群来处理</strong></li></ul><h4 id="混合模型："><a href="#混合模型：" class="headerlink" title="混合模型："></a>混合模型：</h4><ul><li><strong>多种分布式模型组合，例如岛屿+主从模型等</strong></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8A%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%A8%A1%E5%9E%8B%EF%BC%9A%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/5.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>岛主从模型：</strong><ul><li><strong>种群划分成多个子种群，在不同的节点定期进行迁移通信</strong></li><li><strong>对于每个子群，每个主节点会把单独的评估任务发送到从节点</strong></li><li><strong>优点：减少了主从模型中单个主节点的依赖性</strong></li></ul></li><li><strong>岛细胞模型：</strong><ul><li><strong>如图所示，提高了扩展性和容错性</strong></li></ul></li><li><strong>岛岛模型：</strong><ul><li><strong>实现两种迁移方式：可在本地迁移和全局迁移</strong></li><li><strong>优点：提高了每个节点的效率</strong></li></ul></li></ul><h4 id="泳池模型："><a href="#泳池模型：" class="headerlink" title="泳池模型："></a>泳池模型：</h4><ul><li><strong>泳池是一个长度为n的全局共享数组，表示种群中n个个体。然后根据节点个数划分泳池成多个段，每个段对应一个节点。每个节点可以读取任意段中的个体，但是只能往自己段里写。</strong></li><li><strong>优化过程中，节点可以随机选不同段中的个体进行操作，如果生成的后代比自己段中的适应度更好就写回自己的段中</strong></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8A%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%A8%A1%E5%9E%8B%EF%BC%9A%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/6.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>难点在于如何实现一个资源池，如数据库系统，MapReduce等</strong></li></ul><h4 id="协同进化模型："><a href="#协同进化模型：" class="headerlink" title="协同进化模型："></a>协同进化模型：</h4><ul><li><strong>根据节点数量划分每个节点上的向量，确定主变量，其余为从变量</strong></li><li><strong>在主变量上进行进化操作，从变量不变。评估时计算整个向量的适应度。通信阶段，更新其次变量</strong></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8A%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%A8%A1%E5%9E%8B%EF%BC%9A%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/7.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><h4 id="多代理模型："><a href="#多代理模型：" class="headerlink" title="多代理模型："></a>多代理模型：</h4><ul><li><strong>把分布式进化模型视为n个玩家玩策略游戏的系统，每个玩家有一个收益函数，该函数取决于自己和有限邻居的行为，每个玩家都会选择最大收益的行为</strong></li></ul><hr><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p><strong>本文是一篇综述，对每个分布式进化模型及其特点都做了简要概述和点评</strong></p><hr><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><strong>[29] B. Dorronsoro, G. Danoy, A J. Nebro, P. Bouvry, Achieving super-linear per formance in parallel multi-objective evolutionary algorithms by means of cooperative coevolution, Comput. Oper Res. 40(6)(2013)1552-1563</strong><br><strong>[44] M. Garcia-arenas, J-J. Merelo, A M. Mora, P. Castillo, G. Romero, J. L, ]. Laredo. Assessing speed-ups in commodity cloud storage services for distributed evo lutionary algorithms, in: IEEE Congress on Evolutionary Computation(CEC), 2011,pp.304-3</strong><br><strong>[65] F.M.Johar, F.A. Azmin, M K. Suaidi, A.S. Shibghatullah, B H Ahmad, S.N.Salleh,M.Z. AA. Aziz, M. Md Shukor, A review of genetic algorithms and parallel genetic algorithms on graphics processing unit(GPU), in: 2013 IEEE Interna tional Conference on Control System, Computing and Engineering (CCSCE) 2013,pp.264-269</strong><br><strong>[72] X. Li, X Yao, Cooperatively coevolving particle swarms for large scale opti mization, IEEE Trans. Evol. Comput. 16(2)(2012)210-224</strong><br><strong>[97] M. Pedemonte, S. Nesmachnow, H. Cancela, A survey on parallel ant colony optimization, Appl. Soft Comput. 11(8)(2011)5181-5197</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;&lt;a href=&quot;#基本信息&quot; class=&quot;headerlink&quot; title=&quot;基本信息&quot;&gt;&lt;/a&gt;基本信息&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;题目：Distributed evolutionary algorithms and their</summary>
      
    
    
    
    <category term="硕士每周论文笔记" scheme="https://mactql.github.io/categories/%E7%A1%95%E5%A3%AB%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="遗传算法" scheme="https://mactql.github.io/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/"/>
    
    <category term="分布式" scheme="https://mactql.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>剖析HDFS/MR小文件与数据倾斜问题</title>
    <link href="https://mactql.github.io/posts/2599323417.html"/>
    <id>https://mactql.github.io/posts/2599323417.html</id>
    <published>2021-09-24T04:56:00.000Z</published>
    <updated>2021-09-24T04:58:41.450Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是小文件问题？"><a href="#什么是小文件问题？" class="headerlink" title="什么是小文件问题？"></a>什么是小文件问题？</h2><ul><li><strong>HDFS上如果小文件很多，每个小文件都会在NameNode中占用150字节的内存空间</strong></li><li><strong>而在MR中每个小文件都会占一个block，每个block都会产生数据分片对应一个Map任务，导致Map任务特别多，消耗了很多启动Map任务的性能</strong></li></ul><h2 id="如何解决小文件问题？"><a href="#如何解决小文件问题？" class="headerlink" title="如何解决小文件问题？"></a>如何解决小文件问题？</h2><p><strong>HDFS提供了两种容器，SequenceFile 和 MapFile</strong></p><ul><li><strong>SequenceFile</strong><ul><li><strong>是一种二进制文件，会直接把&lt;key,Value&gt;的形式序列化到文件中</strong></li><li><strong>所以，我们可以把小文件进行合并成键值对，Key为文件名，文件内容作为Value，这样序列化组成一个大文件</strong></li><li><strong>缺点：合并后不容易查看，需要通过遍历才能看每个小文件</strong></li></ul></li><li><strong>MapFile</strong><ul><li><strong>是排序后的SequenceFile</strong></li><li><strong>MapFile有两部分，分别是index和data。其中index记录key，以及data在文件中的偏移量。</strong></li><li><strong>优点：查询文件时，可以通过index快速找到数据位置</strong></li></ul></li></ul><hr><h2 id="什么叫数据倾斜问题？"><a href="#什么叫数据倾斜问题？" class="headerlink" title="什么叫数据倾斜问题？"></a>什么叫数据倾斜问题？</h2><ul><li><strong>比如文件里有1000w条数据，里面都是1和0，但是1的数据有900w条，0的数据只有100w条，所以1就是数据倾斜了，这样造成的结果就是处理1的Reduce任务很慢很慢，处理0的Reduce任务早就好了</strong></li><li><strong>总结：MR任务执行时，大部分Reduce节点都处理完毕，但有一个或几个Reduce任务很慢很慢，导致整个Reduce任务很慢</strong></li></ul><h2 id="怎么解决数据倾斜问题？"><a href="#怎么解决数据倾斜问题？" class="headerlink" title="怎么解决数据倾斜问题？"></a>怎么解决数据倾斜问题？</h2><p><strong>有两种解决方案：</strong></p><ul><li><strong>增加Reduce任务个数：治标不治本</strong></li><li><strong>把倾斜的数据打散：比如1倾斜，可以把1改成1_0、1_1等，分到多个Reduce中即可</strong></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是小文件问题？&quot;&gt;&lt;a href=&quot;#什么是小文件问题？&quot; class=&quot;headerlink&quot; title=&quot;什么是小文件问题？&quot;&gt;&lt;/a&gt;什么是小文件问题？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;HDFS上如果小文件很多，每个小文件都会在NameNod</summary>
      
    
    
    
    <category term="《拿来就用的企业级解决方案》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8A%E6%8B%BF%E6%9D%A5%E5%B0%B1%E7%94%A8%E7%9A%84%E4%BC%81%E4%B8%9A%E7%BA%A7%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="HDFS" scheme="https://mactql.github.io/tags/HDFS/"/>
    
    <category term="MapReduce" scheme="https://mactql.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>《Hadoop和Spark上遗传算法分布式架构》论文笔记</title>
    <link href="https://mactql.github.io/posts/2233902538.html"/>
    <id>https://mactql.github.io/posts/2233902538.html</id>
    <published>2021-09-21T08:24:00.000Z</published>
    <updated>2021-09-27T02:44:42.784Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><ul><li><strong>题目：Parallel and distributed architecture of genetic algorithm on Apache Hadoop and Spark</strong></li><li><strong>作者：Hao-chun Lu，F.J.Hwang，Yao-Huei Huang</strong></li><li><strong>期刊：Applied Soft Computing Journal 95(2020) 106497</strong></li><li><strong>时间：2020.06</strong></li><li><strong>链接：</strong>[<a href="https://webvpn1.jiangnan.edu.cn/https/77726476706e69737468656265737421e7e056d234336155700b8ca891472636a6d29e640e/science/article/pii/S1568494620304361]">https://webvpn1.jiangnan.edu.cn/https/77726476706e69737468656265737421e7e056d234336155700b8ca891472636a6d29e640e/science/article/pii/S1568494620304361]</a></li><li><strong>关键字：遗传算法；并行与分布式计算；Apache Hadoop；Apache Spark</strong></li></ul><hr><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li><strong>如果遗传算子所需的迭代过程可以在并行分布式计算体系结构中实现，那么遗传算法在解决大规模优化问题方面的效率将得到提高</strong></li><li><strong>开发的计算框架以将 GA核心操作符 分配到 Apache Hadoop 中的复杂机制为特点，与云计算模型相匹配</strong></li><li><strong>所提出的体系结构可以很容易地扩展到 Apache Spark</strong></li></ul><hr><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><ul><li><strong>文章提出GAN分布式计算体系结构的并行化，是一种基于Apache的遗传算法并行分布式体系结构</strong><ul><li><strong>通过使用相对高效的并行化机制调度遗传算法的核心操作符，提高了HDFS的利用率，减少HDFS作业处理的空闲时间，并且这个并行化机制可以扩展到Apache Spark上的RDD</strong></li></ul></li></ul><hr><h3 id="前人设计的架构"><a href="#前人设计的架构" class="headerlink" title="前人设计的架构"></a>前人设计的架构</h3><ul><li><strong>Verma设计的并行架构：</strong></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8AHadoop%E5%92%8CSpark%E4%B8%8A%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>Mapper用来评估适应度，Reducer用来选择/交叉/变异</strong></li><li><strong>缺点：</strong><ul><li><strong>Reducer中进化不考虑全种群，种群多样性不足会过早收敛</strong></li><li><strong>Mapper中评估适应度后把染色体分区后传给多个Reducer，每个Reducer上的染色体GA轮盘赌概率会变化，导致选择结果不准确，如下图所示</strong></li></ul></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8AHadoop%E5%92%8CSpark%E4%B8%8A%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>​Kečo 和 Subasi设计的并行架构：</strong></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8AHadoop%E5%92%8CSpark%E4%B8%8A%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>第一次迭代，在Mapper中首先评估初始染色体的适应度，然后交叉/变异后再评估一次，然后进行选择</strong></li><li><strong>第二次迭代开始，每次迭代不执行E0，在交叉/变异后评估适应度并判断评估结果。如果满足条件直接输出，不满足则继续迭代</strong></li><li><strong>缺点：</strong><ul><li><strong>没有并行处理GA算子</strong></li></ul></li></ul><hr><h3 id="本文提出的架构"><a href="#本文提出的架构" class="headerlink" title="本文提出的架构"></a>本文提出的架构</h3><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8AHadoop%E5%92%8CSpark%E4%B8%8A%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/3.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>步骤如下：</strong><ol><li><strong>首先生成初始种群，转换成子种群并激活MapReduce1</strong></li><li><strong>在MapReduce1中把每个子种群分配给多个Mapper进行评估适应度</strong></li><li><strong>激活MapReduce2，准备多个交叉池（从整个子种群中选择，组成多个交叉池，防止过早收敛）</strong></li><li><strong>迭代过程：</strong><ol><li><strong>每个Mapper和Reducer对应，依次执行交叉、变异、评估，如上图所示，将结果返回到主程序</strong></li><li><strong>主程序对所有从Reducer收集到的染色体执行选择算子</strong></li><li><strong>判断停止条件，不满足则继续迭代</strong></li></ol></li></ol></li><li><strong>优点：</strong><ul><li><strong>选择算子在主程序中</strong></li><li><strong>交叉/变异同时并行执行，产生的任何染色体都直接在Reducer中评估</strong></li></ul></li></ul><h3 id="将Hadoop并行架构用到Spark上"><a href="#将Hadoop并行架构用到Spark上" class="headerlink" title="将Hadoop并行架构用到Spark上"></a>将Hadoop并行架构用到Spark上</h3><p><img "" class="lazyload placeholder" data-original="/medias/%E3%80%8AHadoop%E5%92%8CSpark%E4%B8%8A%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/4.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>原理同Hadoop</strong></li></ul><hr><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li><strong>评估算子不需要其他染色体信息，独立工作，可以并行化</strong></li><li><strong>交叉算子需要在当前整个种群中选出两条染色体，不然会过早收敛。并且需要设计额外程序保证交叉后的染色体是可行解</strong></li><li><strong>变异算子调整交叉后的染色体保持多样性，也需要额外程序保证变异后是可行解</strong></li><li><strong>选择算子不能在多个子种群中进行选择，有必要把多个Reducer评估后的染色体收集起来形成一个新种群再选择，否则会导致出现Verma架构的缺点</strong></li></ol><ul><li><strong>设计的并行GA架构要满足上述四条</strong></li></ul><hr><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><strong>[19] K. Gallagher, M. Sambridge, Genetic algorithms: a powerful tool for largescale nonlinear optimization problems, Comput. Geosci. 20 (7) (1994) 1229–1236.</strong><br><strong>[22] G. Luque, E. Alba, Parallel models for genetic algorithms, in: Parallel Genetic Algorithms: Theory and Real World Applications, Vol. 367, 2011.</strong><br><strong>[24] L.D. Geronimo, F. Ferrucci, A. Murolo, V. Sarro, A parallel genetic algorithm based on Hadoop MapReduce for the automatic generation of JUnit test suites, in: The IEEE 5th International Conference on Software Testing, Verification and Validation, 2012, pp. 785–793.</strong><br><strong>[26] W. Yu, W. Zhang, Study on function optimization based on master–slave structure genetic algorithm. in: The 8th International Conference on Signal Processing, 2006, pp. 3.</strong><br><strong>[27] N. Melab, E.G. Talbi, GPU-based island model for evolutionary algorithms, in: Proceedings of the 12 Annual Conference on Genetic and Evolutionary Computation, 2010, pp. 1089–1096.</strong><br><strong>[29] S. Arora, I. Chana, A survey of clustering techniques for big data analysis, in: The 5th International Conference - The Next Generation Information Technology Summit Confluence, 2014, pp. 59–65.</strong><br><strong>[30] D. Camacho, Bio-inspired clustering: basic features and future trends in the era of big data, in: 2015 IEEE 2nd International Conference on Cybernetics, CYBCONF, pp. 1–6.</strong><br><strong>[31] Y.J. Gong, W.N. Chen, Z.H. Zhan, J. Zhang, Y. Li, Q. Zhang, J.J. Li, Distributed evolutionary algorithm and their models: A survey of state-of-the-art, Appl. Soft Comput. 34 (2015) 286–300.</strong><br><strong>[33] J. Dean, S. Ghemawat, MapReduce: Simplified data processing on large clusters, Commun. ACM 51 (1) (2008) 107–113.</strong><br><strong>[37] A. Verma, X. Llorà, D.E. Goldberg, R.H. Campbell, Scaling genetic algorithms using MapReduce, in: The 9th IEEE International Conference Intelligent Systems Design and Applications, ISDA’09, 2009, pp. 13–18.</strong><br><strong>[38] D. Kečo, A. Subasi, Parallelization of genetic algorithms using Hadoop Map/Reduce, SouthEast Eur. J. Soft Comput. 1 (2) (2012).</strong><br><strong>[39] R.Z. Qi, Z.J. Wang, S.Y. Li, A parallel genetic algorithm based on spark for pairwise test suite generation, J. Comput. Sci. Tech. 31 (2) (2016) 417–427.</strong><br><strong>[42] R. Gu, X. Yang, J. Yan, Y. Sun, B. Wang, C. Yuan, Y. Huang, SHadoop: improving mapreduce performance by optimizing job execution mechanism in Hadoop clusters, J. Parallel Distrib. Comput. 74 (3) (2014) 2166–2179.</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;&lt;a href=&quot;#基本信息&quot; class=&quot;headerlink&quot; title=&quot;基本信息&quot;&gt;&lt;/a&gt;基本信息&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;题目：Parallel and distributed architecture of gene</summary>
      
    
    
    
    <category term="硕士每周论文笔记" scheme="https://mactql.github.io/categories/%E7%A1%95%E5%A3%AB%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="遗传算法" scheme="https://mactql.github.io/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/"/>
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
    <category term="MapReduce" scheme="https://mactql.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>Redis数据类型</title>
    <link href="https://mactql.github.io/posts/950473165.html"/>
    <id>https://mactql.github.io/posts/950473165.html</id>
    <published>2021-09-17T02:48:00.000Z</published>
    <updated>2021-09-17T02:50:02.058Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Redis常见数据类型"><a href="#Redis常见数据类型" class="headerlink" title="Redis常见数据类型"></a>Redis常见数据类型</h2><p><strong>上一章我们了解了Redis常见的五种数据类型，string、set、hash、sortedset、list，这里我们详细介绍一下</strong></p><hr><h3 id="首先是string"><a href="#首先是string" class="headerlink" title="首先是string"></a>首先是string</h3><p><strong>string可以存任何形式的内容，甚至是二进制数据或图片</strong><br><img "" class="lazyload placeholder" data-original="/medias/Redis%E6%A0%B8%E5%BF%83%E5%AE%9E%E8%B7%B5/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"><br><strong>在这些操作的基础上还有一次添加多个：mset 和一次查多个： mget</strong></p><hr><h3 id="hash类型"><a href="#hash类型" class="headerlink" title="hash类型"></a>hash类型</h3><p><strong>hash类型存的是字段和字段值的映射，类似是在存键值对，只能是字符串，常用来存对象</strong><br><img "" class="lazyload placeholder" data-original="/medias/Redis%E6%A0%B8%E5%BF%83%E5%AE%9E%E8%B7%B5/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"><br><strong>这里的有三个属性，key、field、name，其中field和name是key的value，而field是字段value是字段值，相当于是把一个键值对存在redis中，key是这个键值对的key</strong></p><hr><h2 id="list类型"><a href="#list类型" class="headerlink" title="list类型"></a>list类型</h2><p><strong>list是一个有序的字符串列表，而且是双向链表，常用来当作队列使用</strong><br><img "" class="lazyload placeholder" data-original="/medias/Redis%E6%A0%B8%E5%BF%83%E5%AE%9E%E8%B7%B5/2.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="set类型"><a href="#set类型" class="headerlink" title="set类型"></a>set类型</h2><p><strong>set集合中的元素都是不重复且无序的</strong><br><img "" class="lazyload placeholder" data-original="/medias/Redis%E6%A0%B8%E5%BF%83%E5%AE%9E%E8%B7%B5/3.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="sortedset类型"><a href="#sortedset类型" class="headerlink" title="sortedset类型"></a>sortedset类型</h2><p><strong>与set不同的是有序集合，相当于为每个元素指定一个分数，常用于获取topN的场景</strong><br><img "" class="lazyload placeholder" data-original="/medias/Redis%E6%A0%B8%E5%BF%83%E5%AE%9E%E8%B7%B5/4.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"><br><strong>注意用zadd添加元素要<code>zadd key score value</code> 这样写，一定要在value前给他一个分数</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Redis常见数据类型&quot;&gt;&lt;a href=&quot;#Redis常见数据类型&quot; class=&quot;headerlink&quot; title=&quot;Redis常见数据类型&quot;&gt;&lt;/a&gt;Redis常见数据类型&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;上一章我们了解了Redis常见的五种数据类型，str</summary>
      
    
    
    
    <category term="《快速上手内存数据库Redis》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8A%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%86%85%E5%AD%98%E6%95%B0%E6%8D%AE%E5%BA%93Redis%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://mactql.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>快速了解Redis</title>
    <link href="https://mactql.github.io/posts/4199058362.html"/>
    <id>https://mactql.github.io/posts/4199058362.html</id>
    <published>2021-09-17T02:00:00.000Z</published>
    <updated>2021-09-17T02:02:45.182Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是Redis？"><a href="#什么是Redis？" class="headerlink" title="什么是Redis？"></a>什么是Redis？</h2><ul><li><strong>Redis是一个高性能的基于内存的Key-Value数据库</strong></li><li><strong>可以在N多条记录中根据条件非常快的查找一条或几条记录</strong></li></ul><hr><h2 id="Redis的数据格式是什么样的？"><a href="#Redis的数据格式是什么样的？" class="headerlink" title="Redis的数据格式是什么样的？"></a>Redis的数据格式是什么样的？</h2><ul><li><strong>Redis数据格式为Key-Value</strong><ul><li><strong>Key：String</strong></li><li><strong>Value：String、Hash、List、Set、SortedSet……</strong></li></ul></li></ul><hr><h2 id="Redis应用场景有哪些？"><a href="#Redis应用场景有哪些？" class="headerlink" title="Redis应用场景有哪些？"></a>Redis应用场景有哪些？</h2><ul><li><strong>最常用来当作缓存系统：当用户需要向数据库取数据时，先看redis有没有，没有就去数据库里拿到redis中再从redis中取数据</strong></li><li><strong>计数器：新浪微博的评论数、点赞数</strong></li><li><strong>消息队列：不过用Kafka比较多了</strong></li></ul><hr><h2 id="Redis基础命令有哪些？"><a href="#Redis基础命令有哪些？" class="headerlink" title="Redis基础命令有哪些？"></a>Redis基础命令有哪些？</h2><p><img "" class="lazyload placeholder" data-original="/medias/%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3Redis/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//添加一个键值对</span></span><br><span class="line">set【key】【value】 <span class="comment">//例如set a 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//获取指定key的value</span></span><br><span class="line"><span class="type">Keys</span>【key】<span class="comment">//例如Keys a</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//删除指定键值对</span></span><br><span class="line">del【key】<span class="comment">//例如del a</span></span><br></pre></td></tr></table></figure><hr><h2 id="Redis多数据库特性"><a href="#Redis多数据库特性" class="headerlink" title="Redis多数据库特性"></a>Redis多数据库特性</h2><p><strong>Redis有16个数据库【0-15】，默认在0号数据库，可以用select n 来指定数据库</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是Redis？&quot;&gt;&lt;a href=&quot;#什么是Redis？&quot; class=&quot;headerlink&quot; title=&quot;什么是Redis？&quot;&gt;&lt;/a&gt;什么是Redis？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Redis是一个高性能的基于内存的Key-Value数据</summary>
      
    
    
    
    <category term="《快速上手内存数据库Redis》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8A%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%86%85%E5%AD%98%E6%95%B0%E6%8D%AE%E5%BA%93Redis%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://mactql.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>共享变量与Cache</title>
    <link href="https://mactql.github.io/posts/4244340064.html"/>
    <id>https://mactql.github.io/posts/4244340064.html</id>
    <published>2021-09-09T13:29:00.000Z</published>
    <updated>2021-09-09T13:34:08.608Z</updated>
    
    <content type="html"><![CDATA[<p><strong>默认情況下，一个算子函数中使用到了某个外部的变量，那么这个变量的值会被拷贝到每个task中，此时每个task只能操作自己的那份变量数据</strong><br><strong>Spark提供了两种共享变量，一种是 Broadcast Variable(广播变量)，另一种是 Accumulator(累加变量)</strong></p><hr><h2 id="Broadcast-Variable（广播变量）"><a href="#Broadcast-Variable（广播变量）" class="headerlink" title="Broadcast Variable（广播变量）"></a>Broadcast Variable（广播变量）</h2><blockquote><p><strong>Broadcast Variable（广播变量）会把指定的变量拷贝一份到每个节点上</strong></p><ul><li><strong>通过调用 SparkContext.broadcast(指定变量) 方法为指定的变量创建 只读 的广播变量，通过 广播变量.value() 方法获取值</strong></li><li><strong>优点：</strong><ul><li><strong>如下图所示，如果不使用广播变量，当map计算时会把外部变量拷贝到每个task中，当一个节点task很多的时候会消耗很多资源。用广播变量的话，每个节点只拷贝一份，大大提高了性能</strong></li></ul></li></ul></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F%E4%B8%8ECache/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="Accumulator（累加器）"><a href="#Accumulator（累加器）" class="headerlink" title="Accumulator（累加器）"></a>Accumulator（累加器）</h2><p><strong>Accumulator 只能 专用于累加，并且除了Drive进程以外，其他进程都不能读取值</strong><br><strong>直接看案例就懂了</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="type">SparkSession</span>.builder().getOrCreate().sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment">//直接用外部变量获取RDD中元素的和</span></span><br><span class="line"><span class="keyword">var</span> sum = <span class="number">0</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).foreach( sum += _)</span><br><span class="line">println(sum)</span><br><span class="line">打印结果: <span class="number">0</span></span><br><span class="line">原因是外部变量是在<span class="type">Drive</span>进程中的，用foreach算子计算的和是局部变量传不到<span class="type">Drive</span>，在<span class="type">Drive</span>中println是打印不出来的</span><br><span class="line"></span><br><span class="line"><span class="comment">//用Accumulator获取RDD中元素的和</span></span><br><span class="line"><span class="keyword">var</span> sum = sc.longAccumulator</span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)).foreach(sum.add(_))</span><br><span class="line">println(sum.value)</span><br><span class="line">打印结果：<span class="number">10</span></span><br><span class="line">在<span class="type">Drive</span>进程中可以调用<span class="type">Accumulator</span>变量.value得到累加结果</span><br></pre></td></tr></table></figure><hr><h2 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h2><p><img "" class="lazyload placeholder" data-original="/medias/%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F%E4%B8%8ECache/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>在未引入Cache时：</strong><ul><li><strong>如图所示，因transformation算子有lazy特性，在action之前不会执行。所以当计算result1时，会走一遍step1-&gt;2-&gt;3，当计算result2时，还会走一遍step1-&gt;2-&gt;3，极大浪费资源。</strong></li></ul></li><li><strong>那么现在引入Cache：</strong><ul><li><strong>在RDD2添加Cache后，计算result2时可以直接从Cache中取出计算过的RDD2即可，无需重复计算RDD2</strong></li></ul></li></ul><p><strong>由此可见，在需要重复调用的RDD上非常有必要添加Cache，直接使用<code>RDDname.cache()</code>即可</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;默认情況下，一个算子函数中使用到了某个外部的变量，那么这个变量的值会被拷贝到每个task中，此时每个task只能操作自己的那份变量数据&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;Spark提供了两种共享变量，一种是 Broadcast Variable(广播变</summary>
      
    
    
    
    <category term="《Spark快速上手》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8ASpark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>RDD开发实战</title>
    <link href="https://mactql.github.io/posts/1890251266.html"/>
    <id>https://mactql.github.io/posts/1890251266.html</id>
    <published>2021-09-09T01:43:00.000Z</published>
    <updated>2021-09-09T13:34:23.218Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何创建RDD？"><a href="#如何创建RDD？" class="headerlink" title="如何创建RDD？"></a>如何创建RDD？</h2><ul><li><p><strong>创建RDD有三种方式</strong></p><ul><li><p><strong>基于集合创建RDD：使用sparkContext的parallelize()方法，第一个参数传入集合，第二个参数传入partition数量。Spark会为每个partition执行一个task</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().getOrCreate()</span><br><span class="line"><span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> rdd = spark.sparkContext.parallelize(arr,<span class="number">4</span>) <span class="comment">//基于Array创建一个4分区的rdd</span></span><br></pre></td></tr></table></figure></li><li><p><strong>基于本地或HDFS文件创建RDD：使用sparkContext的textFile()方法，第一个参数传入文件路径，第二个参数传入partition数量</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;WordCount&quot;</span>).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> text = spark.sparkContext.textFile(<span class="string">&quot;/path/words.txt&quot;</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul><hr><h2 id="Spark中对RDD的操作有哪些？"><a href="#Spark中对RDD的操作有哪些？" class="headerlink" title="Spark中对RDD的操作有哪些？"></a>Spark中对RDD的操作有哪些？</h2><ul><li><strong>在Spark中，对RDD的操作只有两种，Transformation 和 Action</strong></li><li><strong>Transformation</strong><ul><li><strong>是对已有的RDD转化为新的RDD，如flatMap、Map等操作</strong></li><li><strong>lazy特性，在没有执行Action之前，所有的操作都只是得到一个逻辑上的RDD，内存中没有任何数据</strong></li></ul></li></ul><ul><li><strong>Action</strong><ul><li><strong>是对RDD最后的操作，如foreach，reduce，返回结果给Driver进程等操作</strong></li><li><strong>只有当执行到Action代码，才会触发之前所有的Transformation算子的执行</strong></li></ul></li></ul><hr><h2 id="Transformation算子实战"><a href="#Transformation算子实战" class="headerlink" title="Transformation算子实战"></a>Transformation算子实战</h2><p><img "" class="lazyload placeholder" data-original="/medias/RDD%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="type">SparkSession</span>.builder().getOrCreate().sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment">//map算子：集合每个元素乘2</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)).map(_ * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//filter算子：过滤集合中的偶数</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)).filter(_ % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//flatMap算子：把每行字符串拆分成单词</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;ns tql&quot;</span>,<span class="string">&quot;jk tcl&quot;</span>)).flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//groupByKey算子：对&lt;&lt;出生地,姓名&gt;&gt;集合根据出生地分组</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;wuxi&quot;</span>,<span class="string">&quot;ns&quot;</span>),(<span class="string">&quot;shandong&quot;</span>,<span class="string">&quot;jk1&quot;</span>),(<span class="string">&quot;wuxi&quot;</span>,<span class="string">&quot;jk2&quot;</span>))).groupByKey()</span><br><span class="line"></span><br><span class="line"><span class="comment">//reduceByKey算子：对&lt;&lt;word,1&gt;&gt;集合计算每个word出现的次数</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;ns&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;jk&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;ns&quot;</span>,<span class="number">1</span>))).reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line"><span class="comment">//sortByKey算子：对&lt;&lt;收入,姓名&gt;&gt;集合根据收入降序排序</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>((<span class="number">10000</span>,<span class="string">&quot;ns&quot;</span>),(<span class="number">100</span>,<span class="string">&quot;jk&quot;</span>))).sortByKey(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//join算子：对&lt;&lt;姓名，收入&gt;&gt;和&lt;&lt;姓名，出生地&gt;&gt;两个集合基于姓名进行合并</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;ns&quot;</span>,<span class="number">10000</span>),(<span class="string">&quot;jk&quot;</span>,<span class="number">100</span>)).join(sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;ns&quot;</span>,<span class="string">&quot;wuxi&quot;</span>))))</span><br><span class="line">合并结果是：<span class="type">Array</span>((<span class="string">&quot;ns&quot;</span>,(<span class="number">10000</span>,<span class="string">&quot;wuxi&quot;</span>)),(<span class="string">&quot;jk&quot;</span>,<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//distinct算子：去除集合中重复元素</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><hr><h2 id="Action算子实战"><a href="#Action算子实战" class="headerlink" title="Action算子实战"></a>Action算子实战</h2><p><img "" class="lazyload placeholder" data-original="/medias/RDD%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="type">SparkSession</span>.builder().getOrCreate().sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment">//reduce算子：求数组元素的和</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).reduce(_+_)</span><br><span class="line"></span><br><span class="line"><span class="comment">//collect算子：返回RDD中的元素集合</span></span><br><span class="line"><span class="keyword">val</span> res = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).collect()</span><br><span class="line">返回的是：<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//take(n)算子：获取RDD中前2个元素</span></span><br><span class="line"><span class="keyword">val</span> res = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).take(<span class="number">2</span>)</span><br><span class="line">返回的是：<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//count算子：获取RDD元素个数</span></span><br><span class="line"><span class="keyword">val</span> res = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).count()</span><br><span class="line">返回的是：<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//saveAsTextFile算子：保存RDD中元素到HDFS上去</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).saveAsTextFile(<span class="string">&quot;hdfs://hdfs路径&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//countByKey算子：对计算元祖的每个Key出现的次数</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;ns&quot;</span>,<span class="number">100</span>),(<span class="string">&quot;ns&quot;</span>,<span class="number">12</span>),(<span class="string">&quot;jk&quot;</span>,<span class="number">14</span>))).countByKey()</span><br><span class="line">返回的是：(<span class="string">&quot;ns&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;jk&quot;</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//foreach算子：遍历输出RDD元素</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).foreach(println(_))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;如何创建RDD？&quot;&gt;&lt;a href=&quot;#如何创建RDD？&quot; class=&quot;headerlink&quot; title=&quot;如何创建RDD？&quot;&gt;&lt;/a&gt;如何创建RDD？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;创建RDD有三种方式&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
</summary>
      
    
    
    
    <category term="《Spark快速上手》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8ASpark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>配置Spark环境及架构介绍</title>
    <link href="https://mactql.github.io/posts/4118677755.html"/>
    <id>https://mactql.github.io/posts/4118677755.html</id>
    <published>2021-09-07T07:11:00.000Z</published>
    <updated>2021-09-09T13:36:32.442Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何在IDEA中配置Spark开发环境？"><a href="#如何在IDEA中配置Spark开发环境？" class="headerlink" title="如何在IDEA中配置Spark开发环境？"></a>如何在IDEA中配置Spark开发环境？</h2><ol><li><strong>首先自行下载scala，并在IDEA中加入scala的SDK，因为spark2.4.3依赖scala2.11，故这里下载scala2.11.11</strong></li></ol><p><img "" class="lazyload placeholder" data-original="/medias/%E9%85%8D%E7%BD%AESpark%E7%8E%AF%E5%A2%83%E5%8F%8A%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ol start="2"><li><strong>并在pom.xml中添加spark2.4.3的依赖</strong><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><hr><h2 id="下面介绍Spark的Standalone模式的系统架构"><a href="#下面介绍Spark的Standalone模式的系统架构" class="headerlink" title="下面介绍Spark的Standalone模式的系统架构"></a>下面介绍Spark的Standalone模式的系统架构</h2><p><img "" class="lazyload placeholder" data-original="/medias/%E9%85%8D%E7%BD%AESpark%E7%8E%AF%E5%A2%83%E5%8F%8A%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>上图所示即为Standalone模式架构图，下面将详细介绍</strong></li><li><strong>首先需要了解几个概念：</strong><ul><li><strong>MasterNode：该节点上常驻Master进程，负责管理全部WorkerNode</strong></li><li><strong>WorkerNode：该节点上常驻Worker进程，负责管理执行Spark任务</strong></li><li><strong>Spark作业：就是一个Spark程序，例如WordCount.scala</strong></li><li><strong>Drive进程：就是运行Spark程序中main()函数的进程</strong></li><li><strong>Executor：就是Spark计算资源的一个单位，用这个单位来占用集群资源，然后分配具体的task给Executor。在Standalone模式中，启动Executor实际上是启动图中的CoarseGrainedExecutorBackend的JVM进程</strong></li><li><strong>Task：Driver在运行main()函数时，会把一个作业拆成多个task，以线程方式在Executor执行如map算子、reduce算子。每个Executor具有多少个cpu就可以运行多少个task，如图中八个cpu两个Executor，故每个Executor可以并行运行4个task</strong></li></ul></li><li><strong>然后介绍一下流程：</strong><ol><li><strong>启动Spark集群时，Master节点会启动Master进程，Worker节点上启动Worker进程</strong></li><li><strong>接下来就提交作业给Master节点，Master节点会通知Worker节点启动Executor</strong></li><li><strong>分配task到Executor上执行，每个Executor可以执行多个task，每个task启动一个线程来执行</strong></li></ol></li><li><strong>还有一些细节：</strong><ul><li><strong>Worker进程上有一个或多个ExecutorRunner对象，每个对象可以控制一个CoarseGrainedExecutorBackend进程的启动和关闭</strong></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;如何在IDEA中配置Spark开发环境？&quot;&gt;&lt;a href=&quot;#如何在IDEA中配置Spark开发环境？&quot; class=&quot;headerlink&quot; title=&quot;如何在IDEA中配置Spark开发环境？&quot;&gt;&lt;/a&gt;如何在IDEA中配置Spark开发环境？&lt;/h2&gt;&lt;</summary>
      
    
    
    
    <category term="《Apache Spark设计与实现》读书笔记" scheme="https://mactql.github.io/categories/%E3%80%8AApache-Spark%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>初识Spark与工作原理</title>
    <link href="https://mactql.github.io/posts/2693573150.html"/>
    <id>https://mactql.github.io/posts/2693573150.html</id>
    <published>2021-09-06T06:59:00.000Z</published>
    <updated>2021-09-09T13:44:48.983Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求分析："><a href="#需求分析：" class="headerlink" title="需求分析："></a>需求分析：</h2><p><strong>读取文件所有内容，统计每个单词出现的次数</strong></p><hr><h2 id="首先介绍一下如何用Scala在本地运行WordCount"><a href="#首先介绍一下如何用Scala在本地运行WordCount" class="headerlink" title="首先介绍一下如何用Scala在本地运行WordCount"></a>首先介绍一下如何用Scala在本地运行WordCount</h2><ol><li><p><strong>第一步，首先要构建Application的运行环境，Driver创建一个SparkContext</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setAppName(<span class="string">&quot;WordCount&quot;</span>) <span class="comment">//设置作业名称</span></span><br><span class="line">.setMaster(<span class="string">&quot;local&quot;</span>) <span class="comment">//设置在本地运行</span></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)  <span class="comment">//通过Conf参数创建一个SparkContext</span></span><br></pre></td></tr></table></figure></li><li><p><strong>第二步，加载数据并转化成RDD</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lineRDD = sc.textFile(<span class="string">&quot;HDFS路径或者磁盘文件的路径&quot;</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第三步，对数据进行切割，把一行数据切成一个个单词</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> wordsRDD = lineRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>)) <span class="comment">//flatMap使用高阶函数，这里对空格进行分割，处理后形成新的RDD</span></span><br></pre></td></tr></table></figure></li><li><p><strong>第四步，迭代words，把每个word转化成(word，1)的键值对形式</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pairRDD = wordsRDD.map((_,<span class="number">1</span>))</span><br></pre></td></tr></table></figure></li><li><p><strong>第五步，根据Key进行分组聚合统计</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> wordCountRDD = pairRDD.reduceByKey(_ + _)</span><br></pre></td></tr></table></figure></li><li><p><strong>第六步，打印结果并关闭SparkContext</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wordCountRDD.foreach(wordCount=&gt;println(wordCount._1+<span class="string">&quot;--&quot;</span>+wordCount._2))</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;需求分析：&quot;&gt;&lt;a href=&quot;#需求分析：&quot; class=&quot;headerlink&quot; title=&quot;需求分析：&quot;&gt;&lt;/a&gt;需求分析：&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;读取文件所有内容，统计每个单词出现的次数&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;首先介</summary>
      
    
    
    
    <category term="《Spark快速上手》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8ASpark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>大数据处理框架概览</title>
    <link href="https://mactql.github.io/posts/407656541.html"/>
    <id>https://mactql.github.io/posts/407656541.html</id>
    <published>2021-09-04T06:41:00.000Z</published>
    <updated>2021-09-09T13:36:30.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="首先了解一下大数据处理框架的四层结构"><a href="#首先了解一下大数据处理框架的四层结构" class="headerlink" title="首先了解一下大数据处理框架的四层结构"></a>首先了解一下大数据处理框架的四层结构</h2><p><img "" class="lazyload placeholder" data-original="/medias/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"><img "" class="lazyload placeholder" data-original="/medias/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>上图所示，即大数据处理框架四层结构，下面将逐一介绍</strong></li></ul><h3 id="用户层"><a href="#用户层" class="headerlink" title="用户层"></a>用户层</h3><ul><li><strong>这一层主要是准备输入数据、Spark或Hadoop的用户代码、配置参数</strong><ul><li><strong>输入数据：一般以分块形式存在HDFS或者Hbase或数据库中</strong></li><li><strong>用户代码：这里需要了解的是，编写代码后会生成一个Driver程序，将代码提交给集群运行，如下图所示，提交Spark代码后，生成的Driver程序可以广播数据给各个task，并且收集task的运行结果</strong></li></ul></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/2.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><p><strong>配置参数：一种是资源需求参数如资源容器数和Cpu大小等，另一种是数据流参数如数据分片大小和分片个数等见四层结构图即可</strong></p><h3 id="分布式数据并行处理层"><a href="#分布式数据并行处理层" class="headerlink" title="分布式数据并行处理层"></a>分布式数据并行处理层</h3></li><li><p><strong>这一层是把用户提交的应用转化为计算任务，然后调用下一层(资源管理与任务调度层)实现并行执行</strong></p><ul><li><strong>转化过程：MapReduce直接就map-shuffle-reduce，但是Spark不一样，如下图所示</strong></li></ul><ol><li><strong>Spark首先要把Spark代码转化为逻辑处理流程，数据处理流程包括数据单元RDD和数据依赖关系，图中每个数据单元RDD里的圆形是RDD的多个数据分片，正方形指的是输入数据分片</strong></li><li><strong>然后如图对逻辑处理流程进行划分，生成物理执行计划，包含多个stage，每个stage包含多个task，task个数一般就是RDD中数据分片的个数</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/3.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></li></ol></li></ul><h3 id="资源管理与任务调度层"><a href="#资源管理与任务调度层" class="headerlink" title="资源管理与任务调度层"></a>资源管理与任务调度层</h3><ul><li><strong>这一层就是资源管理和任务调度，如下图所示</strong></li><li><strong>对于资源管理来说，Spark部署模式不同这一层的工作就不同</strong><ul><li><strong>这里仅介绍Spark的Standalone模式，其他模式后面章节会详细介绍</strong></li><li><strong>Standalone模式类似于MapReduce。区别在于MapReduce为每个task将要运行时启动一个JVM进程，而Spark是预先启动资源容器(Executor JVM)，然后在task执行时在JVM中启动task线程</strong></li></ul></li><li><strong>任务调度有两种调度器：一种是作业调度器：决定多个作业执行顺序；一种是任务调度器，决定多个task执行顺序。下图所示的是先进先出的任务调度器</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/4.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></li></ul><h3 id="物理执行层"><a href="#物理执行层" class="headerlink" title="物理执行层"></a>物理执行层</h3><ul><li><strong>这一层就是负责启动task</strong></li><li><strong>在Spark中一个作业有很多阶段(stage)，每个stage包含很多task，每个task又对应一个JVM的的线程，一个JVM可以同时运行多个task，所以一个JVM的内存空间由多个task共享</strong></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;首先了解一下大数据处理框架的四层结构&quot;&gt;&lt;a href=&quot;#首先了解一下大数据处理框架的四层结构&quot; class=&quot;headerlink&quot; title=&quot;首先了解一下大数据处理框架的四层结构&quot;&gt;&lt;/a&gt;首先了解一下大数据处理框架的四层结构&lt;/h2&gt;&lt;p&gt;&lt;img &quot;</summary>
      
    
    
    
    <category term="《Apache Spark设计与实现》读书笔记" scheme="https://mactql.github.io/categories/%E3%80%8AApache-Spark%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Scala高级特性</title>
    <link href="https://mactql.github.io/posts/159453455.html"/>
    <id>https://mactql.github.io/posts/159453455.html</id>
    <published>2021-09-02T08:26:00.000Z</published>
    <updated>2021-09-02T08:12:11.669Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Scala高级特性一：模式匹配"><a href="#Scala高级特性一：模式匹配" class="headerlink" title="Scala高级特性一：模式匹配"></a>Scala高级特性一：模式匹配</h2><ul><li><strong>Scala模式匹配类似于Java的switchcase，但是更加强大，甚至可以匹配变量类型、集合元素、有值没值</strong></li><li><strong>语法格式为：变量 match { case值 =&gt; 代码 }</strong><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">首先如何匹配变量类型</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">whichtype</span></span>(<span class="type">A</span>: <span class="class"><span class="keyword">type</span>)</span>&#123; <span class="comment">//匹配变量类型是否是type1、type2、type3</span></span><br><span class="line"><span class="type">A</span> <span class="keyword">match</span>&#123;</span><br><span class="line">  <span class="keyword">case</span> a:type1 =&gt; ...</span><br><span class="line">    <span class="keyword">case</span> b:type2 =&gt; ...</span><br><span class="line">    <span class="keyword">case</span> _:<span class="class"><span class="keyword">type</span>  <span class="title">=&gt;</span> ...</span></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">如何匹配有值没值</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isNull</span></span>(<span class="type">A</span> :<span class="class"><span class="keyword">type</span>)</span>&#123;</span><br><span class="line"><span class="type">A</span> <span class="keyword">match</span>&#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">None</span> : ...</span><br><span class="line">    <span class="keyword">case</span> _ : ....</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="Scala高级特性二：隐式转换"><a href="#Scala高级特性二：隐式转换" class="headerlink" title="Scala高级特性二：隐式转换"></a>Scala高级特性二：隐式转换</h2><ul><li><strong>Scala可以在class或者object中定义隐式转换函数，定义后的对应的实例在需要时会自动转换成另一个类型的实例</strong><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//例如让狗抓老鼠</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">cat</span>(<span class="params">val name:<span class="type">String</span></span>)</span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">catmouse</span></span>()&#123;...&#125; <span class="comment">//抓老鼠是cat类的函数</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">Object</span> dog(<span class="keyword">val</span> name:<span class="type">String</span>)&#123;</span><br><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">dogtocat</span></span>(d : dog) = <span class="keyword">new</span> <span class="type">Cat</span>()</span><br><span class="line">  <span class="keyword">new</span> dog().catmouse() <span class="comment">//因为设置了dogtocat隐式转换函数，所以会自动转换成cat并调用抓老鼠方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Scala高级特性一：模式匹配&quot;&gt;&lt;a href=&quot;#Scala高级特性一：模式匹配&quot; class=&quot;headerlink&quot; title=&quot;Scala高级特性一：模式匹配&quot;&gt;&lt;/a&gt;Scala高级特性一：模式匹配&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scal</summary>
      
    
    
    
    <category term="《7天极速掌握Scala》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8A7%E5%A4%A9%E6%9E%81%E9%80%9F%E6%8E%8C%E6%8F%A1Scala%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Scala" scheme="https://mactql.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Scala函数式编程</title>
    <link href="https://mactql.github.io/posts/496349428.html"/>
    <id>https://mactql.github.io/posts/496349428.html</id>
    <published>2021-09-02T07:26:00.000Z</published>
    <updated>2021-09-02T07:28:44.427Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Scala函数式编程特性一：函数赋值给变量"><a href="#Scala函数式编程特性一：函数赋值给变量" class="headerlink" title="Scala函数式编程特性一：函数赋值给变量"></a>Scala函数式编程特性一：函数赋值给变量</h2><ul><li><strong>把函数赋值给变量,函数名+空格+_</strong><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//首先有一个函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span></span>(<span class="type">A</span>:<span class="type">String</span>)&#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//把函数赋值给变量,空格+_即可</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">A</span> = fun _</span><br><span class="line"><span class="comment">//以后就可以直接用变量名代替函数名</span></span><br><span class="line"><span class="type">A</span>(<span class="string">&quot;abc&quot;</span>)</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="Scala函数式编程特性二：匿名函数"><a href="#Scala函数式编程特性二：匿名函数" class="headerlink" title="Scala函数式编程特性二：匿名函数"></a>Scala函数式编程特性二：匿名函数</h2><ul><li><strong>匿名函数：(参数名:参数类型)=&gt;函数体</strong><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> funname = (<span class="type">A</span>:<span class="type">String</span>)=&gt; println(<span class="type">A</span>)</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="Scala函数式编程特性三：高阶函数"><a href="#Scala函数式编程特性三：高阶函数" class="headerlink" title="Scala函数式编程特性三：高阶函数"></a>Scala函数式编程特性三：高阶函数</h2><ul><li><strong>高阶函数：把函数作为参数传给另一个函数</strong></li><li><strong>定义高阶函数时，高阶函数的参数列表中：</strong><ul><li><strong>要写清楚传入函数的参数，当前函数名:(源函数参数)=&gt;源函数返回值类型</strong></li><li><strong>还要写源函数参数，源函数参数：源函数参数类型</strong><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> funA = (<span class="type">A</span>:<span class="type">String</span>)=&gt; println(<span class="type">A</span>) <span class="comment">//定义匿名函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">funB</span></span>(fun:(<span class="type">String</span>)=&gt;<span class="type">Unit</span>,name:<span class="type">String</span>)&#123;</span><br><span class="line">  fun(name) <span class="comment">//在高阶函数中调用函数</span></span><br><span class="line">&#125; <span class="comment">//定义高阶函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//两种方法调用高阶函数</span></span><br><span class="line">funB(funA,<span class="string">&quot;hello&quot;</span>）<span class="comment">//第一种：直接传函数名调用高阶函数</span></span><br><span class="line">funB((<span class="type">A</span>:<span class="type">String</span>)=&gt; println(<span class="type">A</span>),<span class="string">&quot;hello&quot;</span>) <span class="comment">//第二种：直接传整个匿名函数调用高阶函数</span></span><br><span class="line"></span><br><span class="line">对于匿名函数调用的方法还可以简写：</span><br><span class="line">funB((<span class="type">A</span>)=&gt;println(<span class="type">A</span>),<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">当只有一个参数时还可以 funB(<span class="type">A</span>=&gt;println(<span class="type">A</span>),<span class="string">&quot;hello&quot;</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul><hr><h2 id="常用的一些高阶函数的使用"><a href="#常用的一些高阶函数的使用" class="headerlink" title="常用的一些高阶函数的使用"></a>常用的一些高阶函数的使用</h2><ul><li><p><strong>Map：对集合中每个元素都应用一个函数，返回应用后的元素列表</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//例如把数组全部元素*2</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).map(num=&gt;num*<span class="number">2</span>) 在map高阶函数中传入匿名函数</span><br><span class="line"><span class="comment">//也可以简写成</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).map(_ * <span class="number">2</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>flatMap：首先对每个元素执行Map，但是会把每个元素执行的结果再合并成一个大集合并返回</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//例如把字符串数组中的字符串按照空格切开</span></span><br><span class="line"><span class="type">Array</span>(<span class="string">&quot;hello you&quot;</span>,<span class="string">&quot;hello me&quot;</span>).flatMap(line=&gt;line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="comment">//也可以简写成</span></span><br><span class="line"><span class="type">Array</span>(<span class="string">&quot;hello you&quot;</span>,<span class="string">&quot;hello me&quot;</span>).flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br></pre></td></tr></table></figure></li><li><p><strong>foreach：迭代的意思</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//例如输出数组每个元素</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>).foreach(<span class="type">A</span>=&gt;println(<span class="type">A</span>))</span><br><span class="line"><span class="comment">//也可以简写成</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>).foreach(println(_))</span><br></pre></td></tr></table></figure></li><li><p><strong>filter：按照函数进行过滤操作</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//例如过滤出数组中的偶数</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).filter(<span class="type">A</span> =&gt; <span class="type">A</span> % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line"><span class="comment">//也可以简写成</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).filter(_ % <span class="number">2</span> == <span class="number">0</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>reduceLeft：按照函数从左往右的两两元素进行操作，例如累加、求最大值等</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//求数组的和</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>).reduceLeft((<span class="type">A</span>,<span class="type">B</span>)=&gt;<span class="type">A</span>+<span class="type">B</span>)</span><br><span class="line"><span class="comment">//也可以简写成</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).reduceLeft(_+_)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Scala函数式编程特性一：函数赋值给变量&quot;&gt;&lt;a href=&quot;#Scala函数式编程特性一：函数赋值给变量&quot; class=&quot;headerlink&quot; title=&quot;Scala函数式编程特性一：函数赋值给变量&quot;&gt;&lt;/a&gt;Scala函数式编程特性一：函数赋值给变量&lt;/</summary>
      
    
    
    
    <category term="《7天极速掌握Scala》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8A7%E5%A4%A9%E6%9E%81%E9%80%9F%E6%8E%8C%E6%8F%A1Scala%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Scala" scheme="https://mactql.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Scala面向对象</title>
    <link href="https://mactql.github.io/posts/496349427.html"/>
    <id>https://mactql.github.io/posts/496349427.html</id>
    <published>2021-09-01T07:32:00.000Z</published>
    <updated>2021-09-02T07:26:26.134Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Scala的类和对象几乎和Java一样"><a href="#Scala的类和对象几乎和Java一样" class="headerlink" title="Scala的类和对象几乎和Java一样"></a>Scala的类和对象几乎和Java一样</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Point</span>(<span class="params">xc: <span class="type">Int</span>, yc: <span class="type">Int</span></span>) </span>&#123; <span class="comment">//构造函数是直接放在Class的参数列表里，这里和Java不同</span></span><br><span class="line">   <span class="keyword">var</span> x: <span class="type">Int</span> = xc</span><br><span class="line">   <span class="keyword">var</span> y: <span class="type">Int</span> = yc</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">move</span></span>(dx: <span class="type">Int</span>, dy: <span class="type">Int</span>) &#123;</span><br><span class="line">      x = x + dx</span><br><span class="line">      y = y + dy</span><br><span class="line">      println (<span class="string">&quot;x 的坐标点: &quot;</span> + x);</span><br><span class="line">      println (<span class="string">&quot;y 的坐标点: &quot;</span> + y);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p><strong>还有一种不需要类就可以创建对象的方法，直接用Object关键字，相当于Java的静态类</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span></span>&#123;</span><br><span class="line"><span class="keyword">var</span> age</span><br><span class="line">  ...</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line">使用时不需要<span class="keyword">new</span>，可以直接对象名.成员,如下</span><br><span class="line">println(<span class="type">Person</span>.age)</span><br></pre></td></tr></table></figure></li><li><p><strong>需要注意的是：</strong></p><ul><li><strong>如果object和class同名，他们为伴生关系，可以互相访问私有属性</strong></li><li><strong>可以在伴生object中创建一个apply函数，可以调用这个函数直接得到伴生class的对象实例</strong></li></ul></li></ul><hr><h2 id="Scala也必须要一个main方法才能运行"><a href="#Scala也必须要一个main方法才能运行" class="headerlink" title="Scala也必须要一个main方法才能运行"></a>Scala也必须要一个main方法才能运行</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">mainTest</span> </span>&#123;<span class="comment">//main方法只能定义在object中，不能在class中</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="Scala中的接口trait也比较特殊"><a href="#Scala中的接口trait也比较特殊" class="headerlink" title="Scala中的接口trait也比较特殊"></a>Scala中的接口trait也比较特殊</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">TraitA</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">A</span></span>(x: <span class="type">Int</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">TraitB</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">A</span></span>(x: <span class="type">Int</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">extends</span> <span class="title">TraitA</span> <span class="keyword">with</span> <span class="title">TraitB</span></span>&#123;<span class="comment">//实现用extends，多实现后面用with连接</span></span><br><span class="line"><span class="keyword">override</span> <span class="type">TraitA</span>(x:<span class="type">Int</span>)&#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">TraitB</span>(x:<span class="type">Int</span>)&#123;<span class="comment">//可以写override，也可以不写</span></span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Scala的类和对象几乎和Java一样&quot;&gt;&lt;a href=&quot;#Scala的类和对象几乎和Java一样&quot; class=&quot;headerlink&quot; title=&quot;Scala的类和对象几乎和Java一样&quot;&gt;&lt;/a&gt;Scala的类和对象几乎和Java一样&lt;/h2&gt;&lt;figu</summary>
      
    
    
    
    <category term="《7天极速掌握Scala》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8A7%E5%A4%A9%E6%9E%81%E9%80%9F%E6%8E%8C%E6%8F%A1Scala%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Scala" scheme="https://mactql.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Scala基础语法</title>
    <link href="https://mactql.github.io/posts/2823487899.html"/>
    <id>https://mactql.github.io/posts/2823487899.html</id>
    <published>2021-09-01T01:32:00.000Z</published>
    <updated>2021-09-02T07:26:22.353Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何申明变量和常量"><a href="#如何申明变量和常量" class="headerlink" title="如何申明变量和常量"></a>如何申明变量和常量</h2><ul><li><p><strong>val：常量</strong></p></li><li><p><strong>var：变量</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//数据类型如果不指定，会自动根据表达式来推断</span></span><br><span class="line"><span class="keyword">val</span> answer = <span class="number">0</span></span><br><span class="line"><span class="comment">//也可以指定数据类型</span></span><br><span class="line"><span class="keyword">val</span> answer: <span class="type">Int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="那么Scala有哪些数据类型呢？"><a href="#那么Scala有哪些数据类型呢？" class="headerlink" title="那么Scala有哪些数据类型呢？"></a>那么Scala有哪些数据类型呢？</h3></li><li><p><strong>基本数据类型：Byte,Char,Short,Int,Long,Float,Double,Boolean</strong></p></li><li><p><strong>增强版数据类型：StringOps、RichInt、RichChar、RichDouble等（比基本类型多了上百种功能）</strong></p><ul><li><strong>就以RichInt为例：<code>1.to(10)</code> 相当于Range 1 to 10</strong></li></ul></li></ul><hr><h2 id="Scala的-if-和Java是不同的"><a href="#Scala的-if-和Java是不同的" class="headerlink" title="Scala的 if 和Java是不同的"></a>Scala的 if 和Java是不同的</h2><p><strong>区别在于Scala的if是有返回值的，Java的if是没有的</strong><br><strong>举个例子：<code>val ret = if(18&gt;1) 1 else 0</code> 这里 ret 会接收 if 的返回值 1</strong></p><hr><h2 id="Scala的for和Java也不同，while相同"><a href="#Scala的for和Java也不同，while相同" class="headerlink" title="Scala的for和Java也不同，while相同"></a>Scala的for和Java也不同，while相同</h2><p><strong><code>for(i &lt;- 1 to/until n)</code> 意思就是 i 从1到n/n-1迭代</strong><br><strong>甚至可以迭代字符串中每个字符<code>for(c &lt;- &quot;string&quot;) println(c)</code></strong><br><strong>还有高级for循环：</strong></p><ul><li><strong>if守卫：<code>for(i &lt;- 1 to 10 if i % 2 == 0) println(i)</code> 就是把if判断写在for里面不满足就continue</strong></li><li><strong>yield：<code>for(i &lt;- 1 to 3) yield i * 2</code> 可以得到Vector(2，4，6) 就是在for循环中得到的数据组合成一个集合</strong></li></ul><hr><h2 id="Scala中的数组和Java类似"><a href="#Scala中的数组和Java类似" class="headerlink" title="Scala中的数组和Java类似"></a>Scala中的数组和Java类似</h2><ul><li><strong><code>val array = new Array[type](数组长度)</code></strong></li><li><strong>也可以这样写 <code>val array = Array(&quot;....&quot;,23,&quot;..&quot;)</code></strong></li><li><strong>甚至还有像ArrayList那样的可变长度的数组 <code>val ab = new ArrayBuffer[Int]()</code></strong></li><li><strong>还有一个元祖tuple也很常用，可存不同类型的数据 <code>val t = (..,...,...);</code> 并且可以用<code>t._i</code>来获取指定数据</strong></li></ul><hr><h2 id="Scala中也有集合Set、List、Map"><a href="#Scala中也有集合Set、List、Map" class="headerlink" title="Scala中也有集合Set、List、Map"></a>Scala中也有集合Set、List、Map</h2><ul><li><strong><code>val s = Set(1,2,3)</code>这样就直接创建了一个不可变的Set集合，还有HashSet、LinkedHashSet、SortedSet</strong></li><li><strong><code>val l = List(1,2,3)</code> 这样就直接创建了一个不可变的List集合</strong><ul><li><strong>List有很多方法，如<code>l.head</code>，<code>l.tail</code>，<code>for(i &lt;- l) println(i)</code></strong></li><li><strong>ListBuffer是一个可变型的List集合<code>val lb = scala.collection.mutable.ListBuffer[Int]()</code></strong></li></ul></li><li><strong><code>val m = Map((&quot;A&quot;,1),(&quot;B&quot;,2))</code> 这样就直接创建了一个不可变的Map集合</strong></li><li><strong>需要注意的是：</strong><ul><li><strong>如果创建的是一个可变型的集合，添加删除元素可以直接用 +=、-=</strong></li><li><strong>默认用Set、List、Map创建的都是不可变的集合，可以用<code>Scala.collection.mutable.Set/ListBuffer/Map[type](参数列表)</code></strong></li></ul></li></ul><hr><h2 id="Scala的函数和Java的方法也不一样"><a href="#Scala的函数和Java的方法也不一样" class="headerlink" title="Scala的函数和Java的方法也不一样"></a>Scala的函数和Java的方法也不一样</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//需要返回值的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span></span>(<span class="type">A</span>:<span class="class"><span class="keyword">type</span>,<span class="title">B</span></span>:<span class="class"><span class="keyword">type</span>) </span>= &#123;</span><br><span class="line">  ...</span><br><span class="line">  ...</span><br><span class="line">  <span class="type">A</span>,<span class="type">B</span> 最后一行就是返回值，不需要<span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//还有不需要返回值的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span></span>(<span class="type">A</span>:<span class="class"><span class="keyword">type</span>,<span class="title">B</span></span>:<span class="class"><span class="keyword">type</span>) </span>&#123; <span class="comment">//没有=号就是没有返回值</span></span><br><span class="line">  ...</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;如何申明变量和常量&quot;&gt;&lt;a href=&quot;#如何申明变量和常量&quot; class=&quot;headerlink&quot; title=&quot;如何申明变量和常量&quot;&gt;&lt;/a&gt;如何申明变量和常量&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;val：常量&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;</summary>
      
    
    
    
    <category term="《7天极速掌握Scala》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8A7%E5%A4%A9%E6%9E%81%E9%80%9F%E6%8E%8C%E6%8F%A1Scala%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Scala" scheme="https://mactql.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>初识Spark与工作原理</title>
    <link href="https://mactql.github.io/posts/2693573151.html"/>
    <id>https://mactql.github.io/posts/2693573151.html</id>
    <published>2021-08-30T01:43:00.000Z</published>
    <updated>2021-09-09T13:44:48.984Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是Spark？"><a href="#什么是Spark？" class="headerlink" title="什么是Spark？"></a>什么是Spark？</h2><p><strong>Spark是用来替换MapReduce的，因为它基于<code>内存计算</code>，可以比MapReduce快几十倍几百倍</strong></p><hr><h2 id="Spark怎么和Hadoop结合使用？"><a href="#Spark怎么和Hadoop结合使用？" class="headerlink" title="Spark怎么和Hadoop结合使用？"></a>Spark怎么和Hadoop结合使用？</h2><p><strong>如下图所示，后面几章将着重介绍Spark Core和Spark SQL</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86Spark%E4%B8%8E%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="这里简单介绍Spark的工作原理，后面章节会深入分析"><a href="#这里简单介绍Spark的工作原理，后面章节会深入分析" class="headerlink" title="这里简单介绍Spark的工作原理，后面章节会深入分析"></a>这里简单介绍Spark的工作原理，后面章节会深入分析</h2><p><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86Spark%E4%B8%8E%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><blockquote><p><strong>从简化的工作原理图可以看到：</strong></p><ol><li><strong>HDFS的数据加载到内存中转化为RDD，然后假设RDD被分成三份，分别放到三个节点上去，这样可以并行处理</strong></li><li><strong>中间可以调用多个高阶函数处理后形成新的RDD，然后再通过Map或者其他的一些高阶函数进行处理</strong></li><li><strong>然后把数据存储出去</strong></li></ol></blockquote><hr><h2 id="什么是RDD？"><a href="#什么是RDD？" class="headerlink" title="什么是RDD？"></a>什么是RDD？</h2><blockquote><ul><li><strong>RDD其实是一个抽象概念，弹性分布式数据集。</strong></li><li><strong>RDD这个数据集一般情况是放在内存里的。并且RDD可以分区，每个分区放在集群的不同节点上，从而可以并行操作。另外如果某个节点故障导致那个RDD分区的数据丢了，RDD会自动重新计算这个分区的数据</strong></li><li><strong>通俗来说，RDD的表现形式类似数据库的视图，是抽象的，如下图所示</strong></li></ul></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86Spark%E4%B8%8E%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/2.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="下面简单介绍一下Spark架构原理"><a href="#下面简单介绍一下Spark架构原理" class="headerlink" title="下面简单介绍一下Spark架构原理"></a>下面简单介绍一下Spark架构原理</h2><p><strong>[这里以Spark的standalone集群进行分析]</strong></p><blockquote><p><strong>首先要了解几个概念：</strong></p><ul><li><strong>Drive 进程：就是负责执行我们自己编写的 Spark 程序，他所在的节点可以由我们指定，可以是集群上的任意节点</strong></li><li><strong>Master 进程：集群中主节点启动的进程，负责集群资源管理和监控</strong></li><li><strong>Worker 进程：集群中从节点启动的进程，负责启动其他进程( Executor 进程)来执行任务</strong></li><li><strong>Executor 进程：就是 Worker 进程启动的进程</strong></li><li><strong>Task 线程：由 Executor 进程启动的 线程 ，具体运行的就是一些高阶函数、Map操作</strong></li><li><strong>反注册：Executor 进程告诉 Drive 进程，使得 Drive 进程知道有哪些Executor，方便提交task给他们</strong></li></ul></blockquote><p><strong>​</strong></p><blockquote><p><strong>接下来学习Spark架构原理，如下图所示：(仅作了解即可，之后深入分析)</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86Spark%E4%B8%8E%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/3.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是Spark？&quot;&gt;&lt;a href=&quot;#什么是Spark？&quot; class=&quot;headerlink&quot; title=&quot;什么是Spark？&quot;&gt;&lt;/a&gt;什么是Spark？&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Spark是用来替换MapReduce的，因为它基于&lt;code&gt;内存</summary>
      
    
    
    
    <category term="《Spark快速上手》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8ASpark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>第7章 MapReduce工作机制</title>
    <link href="https://mactql.github.io/posts/2408565548.html"/>
    <id>https://mactql.github.io/posts/2408565548.html</id>
    <published>2021-08-27T12:58:00.000Z</published>
    <updated>2021-08-27T12:59:37.971Z</updated>
    
    <content type="html"><![CDATA[<p><strong>在前面章节，我们简单了解了YARN的工作机制，这一章将详细介绍介绍MapReduce是怎么运行</strong></p><h2 id="MapReduce-YARN的工作机制？"><a href="#MapReduce-YARN的工作机制？" class="headerlink" title="MapReduce YARN的工作机制？"></a>MapReduce YARN的工作机制？</h2><p><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC7%E7%AB%A0MapReduce%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><blockquote><p><strong>结合上图来学习一下每个步骤：</strong></p><ol><li><strong>首先是提交作业，直接调用 Job 对象的 submit() 即可，他会创建一个 JobSummiter 实例</strong></li><li><strong>然后这个 JoSummiter 实例会申请 ResourceManager 给这个作业一个 ID ，ResourceManager 会检查一下</strong></li><li><strong>然后把需要的资源复制到 HDFS 的文件名是 ID 号的目录下</strong></li><li><strong>然后 ResourceManager 调用 submitApplication() 方法提交作业</strong></li><li><strong>ResourceManager 分配第一个容器 Container；然后 ResourceManager 在 NodeManager 的管理下，在容器中运行作业的 AppMaster 进程</strong></li><li><strong>AppMaster 对作业进行初始化</strong></li><li><strong>然后 AppMaster 接收输入数据分片，为每个分片创建一个 Map 任务对象</strong></li><li><strong>如果是小作业，就直接在自己节点的JVM中运行。大作业的话就向 ResourceManager 申请 Map 任务和 Reduce 任务的容器 Container</strong></li><li><strong>分配好容器后，AppMaster 就会和这些容器所在节点的 nodemanager 通信来启动这些容器</strong></li><li><strong>在运行 task 之前，还要接收数据放到自己的本地磁盘上</strong></li><li><strong>最后运行 Map 任务或者 Reduce 任务</strong></li></ol></blockquote><hr><h2 id="其实在Map和Reduce之间还有一层shuffle"><a href="#其实在Map和Reduce之间还有一层shuffle" class="headerlink" title="其实在Map和Reduce之间还有一层shuffle"></a>其实在Map和Reduce之间还有一层shuffle</h2><blockquote><p><strong>什么叫shuffle？</strong></p><ul><li><strong>在 Reduce 操作之前，需要保证数据过来都是按照 Key 排序的</strong></li><li><strong>而把 Map 输出的数据按照 Key 排序并输入给 Reduce 的过程就叫 Shuffle</strong></li></ul></blockquote><p><strong>那么是怎么进行shuffle的呢？</strong></p><blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC7%E7%AB%A0MapReduce%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"><br><strong>如图所示，绿色的虚线框框就表示的从 Map 输出到 Reduce 输入中间的 Shuffle过程</strong><br><strong>并且中间两个红色虚线框框分别是 Map 端的 shuffle 和 Reduce 端的 shuffle</strong></p></blockquote><blockquote><p><strong>首先是 Map 端的Shuffle</strong></p><ol><li><strong>每个数据分片由Map任务执行完后，不是直接存到磁盘的，会先存到一个环形缓冲区里</strong></li><li><strong>当这个环形缓冲区快存不下了，就会溢出到磁盘上，形成一个一个溢出文件，这里有些细节需要<code>非常注意</code></strong><ol><li><strong>首先，在往磁盘溢出的过程中，会先按照 Reduce 的个数进行分区，目的是用来指定这个数据要交给哪个 Reducer 处理，就像图中 第2步 有红黄两个 Reducer ，就分两个区，把溢出的文件放到相应的分区里</strong></li><li><strong>另外，在每个分区中还会根据 Key 进行排序，来一个排一个</strong></li><li><strong>如果为了优化性能，设置了 Combiner ，也就是提前 Reduce ，也是在分区内进行操作</strong></li></ol></li><li><strong>随着溢出文件越来越多，在Map任务结束之前会把所有溢出文件合并成一个已分区和排序的输出文件，如图中的第3步</strong></li></ol></blockquote><blockquote><p><strong>还有Reduce端的Shuffle</strong></p><ol><li><strong>每个Map完成时间不同，所以一旦某个Map完成，Reduce都会开始复制Map的输出。每个Reducer都会复制对应分区的数据</strong></li><li><strong>如果数据比较少，就放到内存缓冲区，如果放不下了就溢出到磁盘上，和Map端一样不断合并和排序，最后合并成一个按Key排序的文件作为Reduce输入</strong></li></ol></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;在前面章节，我们简单了解了YARN的工作机制，这一章将详细介绍介绍MapReduce是怎么运行&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;MapReduce-YARN的工作机制？&quot;&gt;&lt;a href=&quot;#MapReduce-YARN的工作机制？&quot; class=</summary>
      
    
    
    
    <category term="《Hadoop权威指南》读书笔记" scheme="https://mactql.github.io/categories/%E3%80%8AHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MapReduce" scheme="https://mactql.github.io/tags/MapReduce/"/>
    
    <category term="YARN" scheme="https://mactql.github.io/tags/YARN/"/>
    
  </entry>
  
  <entry>
    <title>第4章 关于YARN</title>
    <link href="https://mactql.github.io/posts/1038205382.html"/>
    <id>https://mactql.github.io/posts/1038205382.html</id>
    <published>2021-08-26T13:58:00.000Z</published>
    <updated>2021-08-27T06:33:45.964Z</updated>
    
    <content type="html"><![CDATA[<h2 id="首先YARN是什么？"><a href="#首先YARN是什么？" class="headerlink" title="首先YARN是什么？"></a>首先YARN是什么？</h2><blockquote><p><strong>在Hadoop1.0的时候，MapReduce的JobTracker负责了太多工作，接收任务是它，资源调度是它，监控TaskTracker还是它，显然不合理</strong><br><strong>所以在hadoop2.0的时候就把资源调度的任务分离出来，让YARN接手这个任务</strong><br><strong>所以YARN就是一个资源调度框架</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC4%E7%AB%A0%E5%85%B3%E4%BA%8EYARN/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p></blockquote><hr><h2 id="YARN是怎么运行的呢？"><a href="#YARN是怎么运行的呢？" class="headerlink" title="YARN是怎么运行的呢？"></a>YARN是怎么运行的呢？</h2><blockquote><p><strong>在介绍YARN运行机制之前，首先要了解几个概念：</strong></p><ul><li><strong>Container：容器，是YARN对资源进行的一层抽象，如把CPU核数、内存等计算资源封装成一个个Container</strong></li><li><strong>ResourceManager：负责资源调度，整个系统只有一个ResourceManager，例如调度刚刚学的Container</strong></li><li><strong>NodeManager：是ResourceManager在每台机器上的代理，负责管理和监控Container</strong></li><li><strong>ApplicationMaster：负责协调运行MapReduce作业，把一个作业拆成多个 task，并向 ResourceManager 申请容器</strong></li></ul></blockquote><h3 id="我们来看一下提交一个作业到YARN中的流程："><a href="#我们来看一下提交一个作业到YARN中的流程：" class="headerlink" title="我们来看一下提交一个作业到YARN中的流程："></a>我们来看一下提交一个作业到YARN中的流程：</h3><p><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC4%E7%AB%A0%E5%85%B3%E4%BA%8EYARN/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><blockquote><p><strong>我们来具体说说每一步的过程：</strong></p><ol><li><strong>首先 Client 向 Yarn 提交 Application，假设是一个 MapReduce 作业</strong></li><li><strong>然后 ResourceManager 向 NodeManager 申请第一个容器，在这个容器里运行对应的 ApplicationMaster 进程</strong></li><li><strong>然后 ApplicationMaster 把这个作业拆成多个 task ，这些 task 可以在一个或多个容器里运行</strong></li><li><strong>然后向 ResourceManager 申请要运行程序的容器，并定时发送心跳</strong></li><li><strong>申请到容器后，ApplicationMaster 把作业发到对应容器的多个 NodeManager 的容器里去运行，这里运行的可能是 Map 任务，也可能是 Reduce 任务</strong></li><li><strong>运行任务的时候会向 ApplicationMaster 发送心跳，汇报情况</strong></li><li><strong>程序运行完成后， ApplicationMaster 再向 ResourceManager 注销并释放容器资源</strong></li></ol></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;首先YARN是什么？&quot;&gt;&lt;a href=&quot;#首先YARN是什么？&quot; class=&quot;headerlink&quot; title=&quot;首先YARN是什么？&quot;&gt;&lt;/a&gt;首先YARN是什么？&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;在Hadoop1.0的时候，Map</summary>
      
    
    
    
    <category term="《Hadoop权威指南》读书笔记" scheme="https://mactql.github.io/categories/%E3%80%8AHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="YARN" scheme="https://mactql.github.io/tags/YARN/"/>
    
  </entry>
  
  <entry>
    <title>第3章 关于HDFS</title>
    <link href="https://mactql.github.io/posts/86411073.html"/>
    <id>https://mactql.github.io/posts/86411073.html</id>
    <published>2021-08-26T07:22:00.000Z</published>
    <updated>2021-08-26T07:39:06.097Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么要用HDFS，优点和缺点是什么？"><a href="#为什么要用HDFS，优点和缺点是什么？" class="headerlink" title="为什么要用HDFS，优点和缺点是什么？"></a>为什么要用HDFS，优点和缺点是什么？</h2><blockquote><p><strong>首先说一下优点：</strong></p><ul><li><strong>可以存超大文件</strong></li><li><strong>一次写入，多次读取</strong></li><li><strong>可运行在廉价集群上，一个节点坏了还能继续运行</strong></li></ul></blockquote><blockquote><p><strong>然后是缺点：</strong></p><ul><li><strong>不能低延迟时间的访问：HDFS是为了高吞吐优化的，如果要低延迟可以用HBase</strong></li><li><strong>不能有大量小文件：因为namenode把文件的元数据都存在内存，大量小文件会给namenode巨大压力</strong></li><li><strong>不能多用户进行写入操作，也不能任意位置修改文件</strong></li></ul></blockquote><hr><h2 id="下面开始介绍HDFS"><a href="#下面开始介绍HDFS" class="headerlink" title="下面开始介绍HDFS"></a>下面开始介绍HDFS</h2><h3 id="首先要了解HDFS数据块"><a href="#首先要了解HDFS数据块" class="headerlink" title="首先要了解HDFS数据块"></a>首先要了解HDFS数据块</h3><blockquote><ul><li><strong>磁盘有一个Block的概念，它是磁盘读写数据的最小单位，HDFS也有Block</strong></li><li><strong>HDFS可以把一个大文件按照Block的大小进行拆解，存到不同的Block上，并且所有的Block不需要在同一个节点上</strong></li><li><strong>每个Block都可以单独做备份，防止节点坏掉数据丢失</strong></li></ul></blockquote><h3 id="然后要了解的是Namenode和Datanode"><a href="#然后要了解的是Namenode和Datanode" class="headerlink" title="然后要了解的是Namenode和Datanode"></a>然后要了解的是Namenode和Datanode</h3><blockquote><ul><li><strong>Namenode是管理员：管理整个文件系统的数结构和元数据，包括HDFS系统快照和日志文件，并且他知道一个文件的全部Block在哪些Datanode上</strong></li><li><strong>Datanode是工作者：他就负责存储和读取这些Block，还会定期向Namenode汇报他存储的Block的列表</strong></li></ul></blockquote><h3 id="最后是最重要的HDFS是怎么读写的？"><a href="#最后是最重要的HDFS是怎么读写的？" class="headerlink" title="最后是最重要的HDFS是怎么读写的？"></a>最后是最重要的HDFS是怎么读写的？</h3><blockquote><p><strong>首先是从读取数据：</strong></p><ol><li><strong>首先client调用DistributedFS的一个实例的open()方法</strong></li><li><strong>然后DistributedFS会调用Namenode获取文件起始块的Datanode地址</strong></li><li><strong>这个时候DistributedFS会返回一个FSDataInputStream的数据流对象</strong></li><li><strong>然后可以反复调用这个对象的read()方法，数据从Datanode传输到客户端</strong></li><li><strong>读取过程中FSDataInputStream会封装一个DFSInputStream去问Namenode下一批数据块的Datanode地址</strong></li><li><strong>读完就close()</strong></li></ol></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC3%E7%AB%A0%E5%85%B3%E4%BA%8EHDFS/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><blockquote><p><strong>然后是写入数据：</strong></p><ol><li><strong>首先client调用DistributedFS实例的create()方法来新建文件</strong></li><li><strong>然后DistributedFS就会请求Namenode创建一个文件，namenode经过一系列检查后创建一条新建记录</strong></li><li><strong>然后DistributedFS向client返回一个FSDataOutputStream对象，这样就可以开始写数据了</strong></li><li><strong>写的过程中，FSDataOutputStream会封装一个DFSOutputStream对象，这个对象会把数据拆成一个个数据包，放到”数据包队列中”</strong></li><li><strong>然后DataStreamer会让一组Datanode分配新的Block来存储这些数据，然后把数据一个个发送到这些Datanode组成的管道</strong></li><li><strong>管道中所有Datanode都收到后会发送ack应答包回来，然后只有收到ack后数据包才会从队列中删除</strong></li><li><strong>写完后close()</strong></li></ol></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC3%E7%AB%A0%E5%85%B3%E4%BA%8EHDFS/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;为什么要用HDFS，优点和缺点是什么？&quot;&gt;&lt;a href=&quot;#为什么要用HDFS，优点和缺点是什么？&quot; class=&quot;headerlink&quot; title=&quot;为什么要用HDFS，优点和缺点是什么？&quot;&gt;&lt;/a&gt;为什么要用HDFS，优点和缺点是什么？&lt;/h2&gt;&lt;bloc</summary>
      
    
    
    
    <category term="《Hadoop权威指南》读书笔记" scheme="https://mactql.github.io/categories/%E3%80%8AHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="HDFS" scheme="https://mactql.github.io/tags/HDFS/"/>
    
  </entry>
  
</feed>
