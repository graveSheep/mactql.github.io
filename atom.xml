<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>mactql的小站</title>
  
  
  <link href="https://mactql.github.io/atom.xml" rel="self"/>
  
  <link href="https://mactql.github.io/"/>
  <updated>2021-09-17T02:02:29.348Z</updated>
  <id>https://mactql.github.io/</id>
  
  <author>
    <name>mactql</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>快速了解Redis</title>
    <link href="https://mactql.github.io/posts/4199058362.html"/>
    <id>https://mactql.github.io/posts/4199058362.html</id>
    <published>2021-09-17T02:00:00.000Z</published>
    <updated>2021-09-17T02:02:29.348Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是Redis？"><a href="#什么是Redis？" class="headerlink" title="什么是Redis？"></a>什么是Redis？</h2><ul><li><strong>Redis是一个高性能的基于内存的Key-Value数据库</strong></li><li><strong>可以在N多条记录中根据条件非常快的查找一条或几条记录</strong></li></ul><hr><h2 id="Redis的数据格式是什么样的？"><a href="#Redis的数据格式是什么样的？" class="headerlink" title="Redis的数据格式是什么样的？"></a>Redis的数据格式是什么样的？</h2><ul><li><strong>Redis数据格式为Key-Value</strong><ul><li><strong>Key：String</strong></li><li><strong>Value：String、Hash、List、Set、SortedSet……</strong></li></ul></li></ul><hr><h2 id="Redis应用场景有哪些？"><a href="#Redis应用场景有哪些？" class="headerlink" title="Redis应用场景有哪些？"></a>Redis应用场景有哪些？</h2><ul><li><strong>最常用来当作缓存系统：当用户需要向数据库取数据时，先看redis有没有，没有就去数据库里拿到redis中再从redis中取数据</strong></li><li><strong>计数器：新浪微博的评论数、点赞数</strong></li><li><strong>消息队列：不过用Kafka比较多了</strong></li></ul><hr><h2 id="Redis基础命令有哪些？"><a href="#Redis基础命令有哪些？" class="headerlink" title="Redis基础命令有哪些？"></a>Redis基础命令有哪些？</h2><p><img "" class="lazyload placeholder" data-original="/medias/%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3Redis/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//添加一个键值对</span></span><br><span class="line">set【key】【value】 <span class="comment">//例如set a 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//获取指定key的value</span></span><br><span class="line"><span class="type">Keys</span>【key】<span class="comment">//例如Keys a</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//删除指定键值对</span></span><br><span class="line">del【key】<span class="comment">//例如del a</span></span><br></pre></td></tr></table></figure><hr><h2 id="Redis多数据库特性"><a href="#Redis多数据库特性" class="headerlink" title="Redis多数据库特性"></a>Redis多数据库特性</h2><p><strong>Redis有16个数据库【0-15】，默认在0号数据库，可以用select n 来指定数据库</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是Redis？&quot;&gt;&lt;a href=&quot;#什么是Redis？&quot; class=&quot;headerlink&quot; title=&quot;什么是Redis？&quot;&gt;&lt;/a&gt;什么是Redis？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Redis是一个高性能的基于内存的Key-Value数据</summary>
      
    
    
    
    <category term="《快速上手内存数据库Redis》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8A%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E5%86%85%E5%AD%98%E6%95%B0%E6%8D%AE%E5%BA%93Redis%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://mactql.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>共享变量与Cache</title>
    <link href="https://mactql.github.io/posts/4244340064.html"/>
    <id>https://mactql.github.io/posts/4244340064.html</id>
    <published>2021-09-09T13:29:00.000Z</published>
    <updated>2021-09-09T13:34:08.608Z</updated>
    
    <content type="html"><![CDATA[<p><strong>默认情況下，一个算子函数中使用到了某个外部的变量，那么这个变量的值会被拷贝到每个task中，此时每个task只能操作自己的那份变量数据</strong><br><strong>Spark提供了两种共享变量，一种是 Broadcast Variable(广播变量)，另一种是 Accumulator(累加变量)</strong></p><hr><h2 id="Broadcast-Variable（广播变量）"><a href="#Broadcast-Variable（广播变量）" class="headerlink" title="Broadcast Variable（广播变量）"></a>Broadcast Variable（广播变量）</h2><blockquote><p><strong>Broadcast Variable（广播变量）会把指定的变量拷贝一份到每个节点上</strong></p><ul><li><strong>通过调用 SparkContext.broadcast(指定变量) 方法为指定的变量创建 只读 的广播变量，通过 广播变量.value() 方法获取值</strong></li><li><strong>优点：</strong><ul><li><strong>如下图所示，如果不使用广播变量，当map计算时会把外部变量拷贝到每个task中，当一个节点task很多的时候会消耗很多资源。用广播变量的话，每个节点只拷贝一份，大大提高了性能</strong></li></ul></li></ul></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F%E4%B8%8ECache/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="Accumulator（累加器）"><a href="#Accumulator（累加器）" class="headerlink" title="Accumulator（累加器）"></a>Accumulator（累加器）</h2><p><strong>Accumulator 只能 专用于累加，并且除了Drive进程以外，其他进程都不能读取值</strong><br><strong>直接看案例就懂了</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="type">SparkSession</span>.builder().getOrCreate().sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment">//直接用外部变量获取RDD中元素的和</span></span><br><span class="line"><span class="keyword">var</span> sum = <span class="number">0</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).foreach( sum += _)</span><br><span class="line">println(sum)</span><br><span class="line">打印结果: <span class="number">0</span></span><br><span class="line">原因是外部变量是在<span class="type">Drive</span>进程中的，用foreach算子计算的和是局部变量传不到<span class="type">Drive</span>，在<span class="type">Drive</span>中println是打印不出来的</span><br><span class="line"></span><br><span class="line"><span class="comment">//用Accumulator获取RDD中元素的和</span></span><br><span class="line"><span class="keyword">var</span> sum = sc.longAccumulator</span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)).foreach(sum.add(_))</span><br><span class="line">println(sum.value)</span><br><span class="line">打印结果：<span class="number">10</span></span><br><span class="line">在<span class="type">Drive</span>进程中可以调用<span class="type">Accumulator</span>变量.value得到累加结果</span><br></pre></td></tr></table></figure><hr><h2 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h2><p><img "" class="lazyload placeholder" data-original="/medias/%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F%E4%B8%8ECache/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>在未引入Cache时：</strong><ul><li><strong>如图所示，因transformation算子有lazy特性，在action之前不会执行。所以当计算result1时，会走一遍step1-&gt;2-&gt;3，当计算result2时，还会走一遍step1-&gt;2-&gt;3，极大浪费资源。</strong></li></ul></li><li><strong>那么现在引入Cache：</strong><ul><li><strong>在RDD2添加Cache后，计算result2时可以直接从Cache中取出计算过的RDD2即可，无需重复计算RDD2</strong></li></ul></li></ul><p><strong>由此可见，在需要重复调用的RDD上非常有必要添加Cache，直接使用<code>RDDname.cache()</code>即可</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;默认情況下，一个算子函数中使用到了某个外部的变量，那么这个变量的值会被拷贝到每个task中，此时每个task只能操作自己的那份变量数据&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;Spark提供了两种共享变量，一种是 Broadcast Variable(广播变</summary>
      
    
    
    
    <category term="《Spark快速上手》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8ASpark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>RDD开发实战</title>
    <link href="https://mactql.github.io/posts/1890251266.html"/>
    <id>https://mactql.github.io/posts/1890251266.html</id>
    <published>2021-09-09T01:43:00.000Z</published>
    <updated>2021-09-09T13:34:23.218Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何创建RDD？"><a href="#如何创建RDD？" class="headerlink" title="如何创建RDD？"></a>如何创建RDD？</h2><ul><li><p><strong>创建RDD有三种方式</strong></p><ul><li><p><strong>基于集合创建RDD：使用sparkContext的parallelize()方法，第一个参数传入集合，第二个参数传入partition数量。Spark会为每个partition执行一个task</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().getOrCreate()</span><br><span class="line"><span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> rdd = spark.sparkContext.parallelize(arr,<span class="number">4</span>) <span class="comment">//基于Array创建一个4分区的rdd</span></span><br></pre></td></tr></table></figure></li><li><p><strong>基于本地或HDFS文件创建RDD：使用sparkContext的textFile()方法，第一个参数传入文件路径，第二个参数传入partition数量</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">&quot;WordCount&quot;</span>).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> text = spark.sparkContext.textFile(<span class="string">&quot;/path/words.txt&quot;</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul><hr><h2 id="Spark中对RDD的操作有哪些？"><a href="#Spark中对RDD的操作有哪些？" class="headerlink" title="Spark中对RDD的操作有哪些？"></a>Spark中对RDD的操作有哪些？</h2><ul><li><strong>在Spark中，对RDD的操作只有两种，Transformation 和 Action</strong></li><li><strong>Transformation</strong><ul><li><strong>是对已有的RDD转化为新的RDD，如flatMap、Map等操作</strong></li><li><strong>lazy特性，在没有执行Action之前，所有的操作都只是得到一个逻辑上的RDD，内存中没有任何数据</strong></li></ul></li></ul><ul><li><strong>Action</strong><ul><li><strong>是对RDD最后的操作，如foreach，reduce，返回结果给Driver进程等操作</strong></li><li><strong>只有当执行到Action代码，才会触发之前所有的Transformation算子的执行</strong></li></ul></li></ul><hr><h2 id="Transformation算子实战"><a href="#Transformation算子实战" class="headerlink" title="Transformation算子实战"></a>Transformation算子实战</h2><p><img "" class="lazyload placeholder" data-original="/medias/RDD%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="type">SparkSession</span>.builder().getOrCreate().sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment">//map算子：集合每个元素乘2</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)).map(_ * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//filter算子：过滤集合中的偶数</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)).filter(_ % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//flatMap算子：把每行字符串拆分成单词</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;ns tql&quot;</span>,<span class="string">&quot;jk tcl&quot;</span>)).flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//groupByKey算子：对&lt;&lt;出生地,姓名&gt;&gt;集合根据出生地分组</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;wuxi&quot;</span>,<span class="string">&quot;ns&quot;</span>),(<span class="string">&quot;shandong&quot;</span>,<span class="string">&quot;jk1&quot;</span>),(<span class="string">&quot;wuxi&quot;</span>,<span class="string">&quot;jk2&quot;</span>))).groupByKey()</span><br><span class="line"></span><br><span class="line"><span class="comment">//reduceByKey算子：对&lt;&lt;word,1&gt;&gt;集合计算每个word出现的次数</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;ns&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;jk&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;ns&quot;</span>,<span class="number">1</span>))).reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line"><span class="comment">//sortByKey算子：对&lt;&lt;收入,姓名&gt;&gt;集合根据收入降序排序</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>((<span class="number">10000</span>,<span class="string">&quot;ns&quot;</span>),(<span class="number">100</span>,<span class="string">&quot;jk&quot;</span>))).sortByKey(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//join算子：对&lt;&lt;姓名，收入&gt;&gt;和&lt;&lt;姓名，出生地&gt;&gt;两个集合基于姓名进行合并</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;ns&quot;</span>,<span class="number">10000</span>),(<span class="string">&quot;jk&quot;</span>,<span class="number">100</span>)).join(sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;ns&quot;</span>,<span class="string">&quot;wuxi&quot;</span>))))</span><br><span class="line">合并结果是：<span class="type">Array</span>((<span class="string">&quot;ns&quot;</span>,(<span class="number">10000</span>,<span class="string">&quot;wuxi&quot;</span>)),(<span class="string">&quot;jk&quot;</span>,<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//distinct算子：去除集合中重复元素</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><hr><h2 id="Action算子实战"><a href="#Action算子实战" class="headerlink" title="Action算子实战"></a>Action算子实战</h2><p><img "" class="lazyload placeholder" data-original="/medias/RDD%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="type">SparkSession</span>.builder().getOrCreate().sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment">//reduce算子：求数组元素的和</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).reduce(_+_)</span><br><span class="line"></span><br><span class="line"><span class="comment">//collect算子：返回RDD中的元素集合</span></span><br><span class="line"><span class="keyword">val</span> res = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).collect()</span><br><span class="line">返回的是：<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//take(n)算子：获取RDD中前2个元素</span></span><br><span class="line"><span class="keyword">val</span> res = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).take(<span class="number">2</span>)</span><br><span class="line">返回的是：<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//count算子：获取RDD元素个数</span></span><br><span class="line"><span class="keyword">val</span> res = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).count()</span><br><span class="line">返回的是：<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//saveAsTextFile算子：保存RDD中元素到HDFS上去</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).saveAsTextFile(<span class="string">&quot;hdfs://hdfs路径&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//countByKey算子：对计算元祖的每个Key出现的次数</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;ns&quot;</span>,<span class="number">100</span>),(<span class="string">&quot;ns&quot;</span>,<span class="number">12</span>),(<span class="string">&quot;jk&quot;</span>,<span class="number">14</span>))).countByKey()</span><br><span class="line">返回的是：(<span class="string">&quot;ns&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;jk&quot;</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//foreach算子：遍历输出RDD元素</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)).foreach(println(_))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;如何创建RDD？&quot;&gt;&lt;a href=&quot;#如何创建RDD？&quot; class=&quot;headerlink&quot; title=&quot;如何创建RDD？&quot;&gt;&lt;/a&gt;如何创建RDD？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;创建RDD有三种方式&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
</summary>
      
    
    
    
    <category term="《Spark快速上手》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8ASpark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>配置Spark环境及架构介绍</title>
    <link href="https://mactql.github.io/posts/4118677755.html"/>
    <id>https://mactql.github.io/posts/4118677755.html</id>
    <published>2021-09-07T07:11:00.000Z</published>
    <updated>2021-09-09T13:36:32.442Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何在IDEA中配置Spark开发环境？"><a href="#如何在IDEA中配置Spark开发环境？" class="headerlink" title="如何在IDEA中配置Spark开发环境？"></a>如何在IDEA中配置Spark开发环境？</h2><ol><li><strong>首先自行下载scala，并在IDEA中加入scala的SDK，因为spark2.4.3依赖scala2.11，故这里下载scala2.11.11</strong></li></ol><p><img "" class="lazyload placeholder" data-original="/medias/%E9%85%8D%E7%BD%AESpark%E7%8E%AF%E5%A2%83%E5%8F%8A%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ol start="2"><li><strong>并在pom.xml中添加spark2.4.3的依赖</strong><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><hr><h2 id="下面介绍Spark的Standalone模式的系统架构"><a href="#下面介绍Spark的Standalone模式的系统架构" class="headerlink" title="下面介绍Spark的Standalone模式的系统架构"></a>下面介绍Spark的Standalone模式的系统架构</h2><p><img "" class="lazyload placeholder" data-original="/medias/%E9%85%8D%E7%BD%AESpark%E7%8E%AF%E5%A2%83%E5%8F%8A%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>上图所示即为Standalone模式架构图，下面将详细介绍</strong></li><li><strong>首先需要了解几个概念：</strong><ul><li><strong>MasterNode：该节点上常驻Master进程，负责管理全部WorkerNode</strong></li><li><strong>WorkerNode：该节点上常驻Worker进程，负责管理执行Spark任务</strong></li><li><strong>Spark作业：就是一个Spark程序，例如WordCount.scala</strong></li><li><strong>Drive进程：就是运行Spark程序中main()函数的进程</strong></li><li><strong>Executor：就是Spark计算资源的一个单位，用这个单位来占用集群资源，然后分配具体的task给Executor。在Standalone模式中，启动Executor实际上是启动图中的CoarseGrainedExecutorBackend的JVM进程</strong></li><li><strong>Task：Driver在运行main()函数时，会把一个作业拆成多个task，以线程方式在Executor执行如map算子、reduce算子。每个Executor具有多少个cpu就可以运行多少个task，如图中八个cpu两个Executor，故每个Executor可以并行运行4个task</strong></li></ul></li><li><strong>然后介绍一下流程：</strong><ol><li><strong>启动Spark集群时，Master节点会启动Master进程，Worker节点上启动Worker进程</strong></li><li><strong>接下来就提交作业给Master节点，Master节点会通知Worker节点启动Executor</strong></li><li><strong>分配task到Executor上执行，每个Executor可以执行多个task，每个task启动一个线程来执行</strong></li></ol></li><li><strong>还有一些细节：</strong><ul><li><strong>Worker进程上有一个或多个ExecutorRunner对象，每个对象可以控制一个CoarseGrainedExecutorBackend进程的启动和关闭</strong></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;如何在IDEA中配置Spark开发环境？&quot;&gt;&lt;a href=&quot;#如何在IDEA中配置Spark开发环境？&quot; class=&quot;headerlink&quot; title=&quot;如何在IDEA中配置Spark开发环境？&quot;&gt;&lt;/a&gt;如何在IDEA中配置Spark开发环境？&lt;/h2&gt;&lt;</summary>
      
    
    
    
    <category term="《Apache Spark设计与实现》读书笔记" scheme="https://mactql.github.io/categories/%E3%80%8AApache-Spark%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>初识Spark与工作原理</title>
    <link href="https://mactql.github.io/posts/2693573150.html"/>
    <id>https://mactql.github.io/posts/2693573150.html</id>
    <published>2021-09-06T06:59:00.000Z</published>
    <updated>2021-09-09T13:44:48.983Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求分析："><a href="#需求分析：" class="headerlink" title="需求分析："></a>需求分析：</h2><p><strong>读取文件所有内容，统计每个单词出现的次数</strong></p><hr><h2 id="首先介绍一下如何用Scala在本地运行WordCount"><a href="#首先介绍一下如何用Scala在本地运行WordCount" class="headerlink" title="首先介绍一下如何用Scala在本地运行WordCount"></a>首先介绍一下如何用Scala在本地运行WordCount</h2><ol><li><p><strong>第一步，首先要构建Application的运行环境，Driver创建一个SparkContext</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setAppName(<span class="string">&quot;WordCount&quot;</span>) <span class="comment">//设置作业名称</span></span><br><span class="line">.setMaster(<span class="string">&quot;local&quot;</span>) <span class="comment">//设置在本地运行</span></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)  <span class="comment">//通过Conf参数创建一个SparkContext</span></span><br></pre></td></tr></table></figure></li><li><p><strong>第二步，加载数据并转化成RDD</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lineRDD = sc.textFile(<span class="string">&quot;HDFS路径或者磁盘文件的路径&quot;</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第三步，对数据进行切割，把一行数据切成一个个单词</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> wordsRDD = lineRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>)) <span class="comment">//flatMap使用高阶函数，这里对空格进行分割，处理后形成新的RDD</span></span><br></pre></td></tr></table></figure></li><li><p><strong>第四步，迭代words，把每个word转化成(word，1)的键值对形式</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pairRDD = wordsRDD.map((_,<span class="number">1</span>))</span><br></pre></td></tr></table></figure></li><li><p><strong>第五步，根据Key进行分组聚合统计</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> wordCountRDD = pairRDD.reduceByKey(_ + _)</span><br></pre></td></tr></table></figure></li><li><p><strong>第六步，打印结果并关闭SparkContext</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wordCountRDD.foreach(wordCount=&gt;println(wordCount._1+<span class="string">&quot;--&quot;</span>+wordCount._2))</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;需求分析：&quot;&gt;&lt;a href=&quot;#需求分析：&quot; class=&quot;headerlink&quot; title=&quot;需求分析：&quot;&gt;&lt;/a&gt;需求分析：&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;读取文件所有内容，统计每个单词出现的次数&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;首先介</summary>
      
    
    
    
    <category term="《Spark快速上手》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8ASpark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>大数据处理框架概览</title>
    <link href="https://mactql.github.io/posts/407656541.html"/>
    <id>https://mactql.github.io/posts/407656541.html</id>
    <published>2021-09-04T06:41:00.000Z</published>
    <updated>2021-09-09T13:36:30.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="首先了解一下大数据处理框架的四层结构"><a href="#首先了解一下大数据处理框架的四层结构" class="headerlink" title="首先了解一下大数据处理框架的四层结构"></a>首先了解一下大数据处理框架的四层结构</h2><p><img "" class="lazyload placeholder" data-original="/medias/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"><img "" class="lazyload placeholder" data-original="/medias/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>上图所示，即大数据处理框架四层结构，下面将逐一介绍</strong></li></ul><h3 id="用户层"><a href="#用户层" class="headerlink" title="用户层"></a>用户层</h3><ul><li><strong>这一层主要是准备输入数据、Spark或Hadoop的用户代码、配置参数</strong><ul><li><strong>输入数据：一般以分块形式存在HDFS或者Hbase或数据库中</strong></li><li><strong>用户代码：这里需要了解的是，编写代码后会生成一个Driver程序，将代码提交给集群运行，如下图所示，提交Spark代码后，生成的Driver程序可以广播数据给各个task，并且收集task的运行结果</strong></li></ul></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/2.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><p><strong>配置参数：一种是资源需求参数如资源容器数和Cpu大小等，另一种是数据流参数如数据分片大小和分片个数等见四层结构图即可</strong></p><h3 id="分布式数据并行处理层"><a href="#分布式数据并行处理层" class="headerlink" title="分布式数据并行处理层"></a>分布式数据并行处理层</h3></li><li><p><strong>这一层是把用户提交的应用转化为计算任务，然后调用下一层(资源管理与任务调度层)实现并行执行</strong></p><ul><li><strong>转化过程：MapReduce直接就map-shuffle-reduce，但是Spark不一样，如下图所示</strong></li></ul><ol><li><strong>Spark首先要把Spark代码转化为逻辑处理流程，数据处理流程包括数据单元RDD和数据依赖关系，图中每个数据单元RDD里的圆形是RDD的多个数据分片，正方形指的是输入数据分片</strong></li><li><strong>然后如图对逻辑处理流程进行划分，生成物理执行计划，包含多个stage，每个stage包含多个task，task个数一般就是RDD中数据分片的个数</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/3.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></li></ol></li></ul><h3 id="资源管理与任务调度层"><a href="#资源管理与任务调度层" class="headerlink" title="资源管理与任务调度层"></a>资源管理与任务调度层</h3><ul><li><strong>这一层就是资源管理和任务调度，如下图所示</strong></li><li><strong>对于资源管理来说，Spark部署模式不同这一层的工作就不同</strong><ul><li><strong>这里仅介绍Spark的Standalone模式，其他模式后面章节会详细介绍</strong></li><li><strong>Standalone模式类似于MapReduce。区别在于MapReduce为每个task将要运行时启动一个JVM进程，而Spark是预先启动资源容器(Executor JVM)，然后在task执行时在JVM中启动task线程</strong></li></ul></li><li><strong>任务调度有两种调度器：一种是作业调度器：决定多个作业执行顺序；一种是任务调度器，决定多个task执行顺序。下图所示的是先进先出的任务调度器</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88/4.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></li></ul><h3 id="物理执行层"><a href="#物理执行层" class="headerlink" title="物理执行层"></a>物理执行层</h3><ul><li><strong>这一层就是负责启动task</strong></li><li><strong>在Spark中一个作业有很多阶段(stage)，每个stage包含很多task，每个task又对应一个JVM的的线程，一个JVM可以同时运行多个task，所以一个JVM的内存空间由多个task共享</strong></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;首先了解一下大数据处理框架的四层结构&quot;&gt;&lt;a href=&quot;#首先了解一下大数据处理框架的四层结构&quot; class=&quot;headerlink&quot; title=&quot;首先了解一下大数据处理框架的四层结构&quot;&gt;&lt;/a&gt;首先了解一下大数据处理框架的四层结构&lt;/h2&gt;&lt;p&gt;&lt;img &quot;</summary>
      
    
    
    
    <category term="《Apache Spark设计与实现》读书笔记" scheme="https://mactql.github.io/categories/%E3%80%8AApache-Spark%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Scala高级特性</title>
    <link href="https://mactql.github.io/posts/159453455.html"/>
    <id>https://mactql.github.io/posts/159453455.html</id>
    <published>2021-09-02T08:26:00.000Z</published>
    <updated>2021-09-02T08:12:11.669Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Scala高级特性一：模式匹配"><a href="#Scala高级特性一：模式匹配" class="headerlink" title="Scala高级特性一：模式匹配"></a>Scala高级特性一：模式匹配</h2><ul><li><strong>Scala模式匹配类似于Java的switchcase，但是更加强大，甚至可以匹配变量类型、集合元素、有值没值</strong></li><li><strong>语法格式为：变量 match { case值 =&gt; 代码 }</strong><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">首先如何匹配变量类型</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">whichtype</span></span>(<span class="type">A</span>: <span class="class"><span class="keyword">type</span>)</span>&#123; <span class="comment">//匹配变量类型是否是type1、type2、type3</span></span><br><span class="line"><span class="type">A</span> <span class="keyword">match</span>&#123;</span><br><span class="line">  <span class="keyword">case</span> a:type1 =&gt; ...</span><br><span class="line">    <span class="keyword">case</span> b:type2 =&gt; ...</span><br><span class="line">    <span class="keyword">case</span> _:<span class="class"><span class="keyword">type</span>  <span class="title">=&gt;</span> ...</span></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">如何匹配有值没值</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isNull</span></span>(<span class="type">A</span> :<span class="class"><span class="keyword">type</span>)</span>&#123;</span><br><span class="line"><span class="type">A</span> <span class="keyword">match</span>&#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">None</span> : ...</span><br><span class="line">    <span class="keyword">case</span> _ : ....</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="Scala高级特性二：隐式转换"><a href="#Scala高级特性二：隐式转换" class="headerlink" title="Scala高级特性二：隐式转换"></a>Scala高级特性二：隐式转换</h2><ul><li><strong>Scala可以在class或者object中定义隐式转换函数，定义后的对应的实例在需要时会自动转换成另一个类型的实例</strong><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//例如让狗抓老鼠</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">cat</span>(<span class="params">val name:<span class="type">String</span></span>)</span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">catmouse</span></span>()&#123;...&#125; <span class="comment">//抓老鼠是cat类的函数</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">Object</span> dog(<span class="keyword">val</span> name:<span class="type">String</span>)&#123;</span><br><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">dogtocat</span></span>(d : dog) = <span class="keyword">new</span> <span class="type">Cat</span>()</span><br><span class="line">  <span class="keyword">new</span> dog().catmouse() <span class="comment">//因为设置了dogtocat隐式转换函数，所以会自动转换成cat并调用抓老鼠方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Scala高级特性一：模式匹配&quot;&gt;&lt;a href=&quot;#Scala高级特性一：模式匹配&quot; class=&quot;headerlink&quot; title=&quot;Scala高级特性一：模式匹配&quot;&gt;&lt;/a&gt;Scala高级特性一：模式匹配&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scal</summary>
      
    
    
    
    <category term="《7天极速掌握Scala》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8A7%E5%A4%A9%E6%9E%81%E9%80%9F%E6%8E%8C%E6%8F%A1Scala%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Scala" scheme="https://mactql.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Scala函数式编程</title>
    <link href="https://mactql.github.io/posts/496349428.html"/>
    <id>https://mactql.github.io/posts/496349428.html</id>
    <published>2021-09-02T07:26:00.000Z</published>
    <updated>2021-09-02T07:28:44.427Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Scala函数式编程特性一：函数赋值给变量"><a href="#Scala函数式编程特性一：函数赋值给变量" class="headerlink" title="Scala函数式编程特性一：函数赋值给变量"></a>Scala函数式编程特性一：函数赋值给变量</h2><ul><li><strong>把函数赋值给变量,函数名+空格+_</strong><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//首先有一个函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span></span>(<span class="type">A</span>:<span class="type">String</span>)&#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//把函数赋值给变量,空格+_即可</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">A</span> = fun _</span><br><span class="line"><span class="comment">//以后就可以直接用变量名代替函数名</span></span><br><span class="line"><span class="type">A</span>(<span class="string">&quot;abc&quot;</span>)</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="Scala函数式编程特性二：匿名函数"><a href="#Scala函数式编程特性二：匿名函数" class="headerlink" title="Scala函数式编程特性二：匿名函数"></a>Scala函数式编程特性二：匿名函数</h2><ul><li><strong>匿名函数：(参数名:参数类型)=&gt;函数体</strong><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> funname = (<span class="type">A</span>:<span class="type">String</span>)=&gt; println(<span class="type">A</span>)</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="Scala函数式编程特性三：高阶函数"><a href="#Scala函数式编程特性三：高阶函数" class="headerlink" title="Scala函数式编程特性三：高阶函数"></a>Scala函数式编程特性三：高阶函数</h2><ul><li><strong>高阶函数：把函数作为参数传给另一个函数</strong></li><li><strong>定义高阶函数时，高阶函数的参数列表中：</strong><ul><li><strong>要写清楚传入函数的参数，当前函数名:(源函数参数)=&gt;源函数返回值类型</strong></li><li><strong>还要写源函数参数，源函数参数：源函数参数类型</strong><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> funA = (<span class="type">A</span>:<span class="type">String</span>)=&gt; println(<span class="type">A</span>) <span class="comment">//定义匿名函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">funB</span></span>(fun:(<span class="type">String</span>)=&gt;<span class="type">Unit</span>,name:<span class="type">String</span>)&#123;</span><br><span class="line">  fun(name) <span class="comment">//在高阶函数中调用函数</span></span><br><span class="line">&#125; <span class="comment">//定义高阶函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//两种方法调用高阶函数</span></span><br><span class="line">funB(funA,<span class="string">&quot;hello&quot;</span>）<span class="comment">//第一种：直接传函数名调用高阶函数</span></span><br><span class="line">funB((<span class="type">A</span>:<span class="type">String</span>)=&gt; println(<span class="type">A</span>),<span class="string">&quot;hello&quot;</span>) <span class="comment">//第二种：直接传整个匿名函数调用高阶函数</span></span><br><span class="line"></span><br><span class="line">对于匿名函数调用的方法还可以简写：</span><br><span class="line">funB((<span class="type">A</span>)=&gt;println(<span class="type">A</span>),<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">当只有一个参数时还可以 funB(<span class="type">A</span>=&gt;println(<span class="type">A</span>),<span class="string">&quot;hello&quot;</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul><hr><h2 id="常用的一些高阶函数的使用"><a href="#常用的一些高阶函数的使用" class="headerlink" title="常用的一些高阶函数的使用"></a>常用的一些高阶函数的使用</h2><ul><li><p><strong>Map：对集合中每个元素都应用一个函数，返回应用后的元素列表</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//例如把数组全部元素*2</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).map(num=&gt;num*<span class="number">2</span>) 在map高阶函数中传入匿名函数</span><br><span class="line"><span class="comment">//也可以简写成</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).map(_ * <span class="number">2</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>flatMap：首先对每个元素执行Map，但是会把每个元素执行的结果再合并成一个大集合并返回</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//例如把字符串数组中的字符串按照空格切开</span></span><br><span class="line"><span class="type">Array</span>(<span class="string">&quot;hello you&quot;</span>,<span class="string">&quot;hello me&quot;</span>).flatMap(line=&gt;line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="comment">//也可以简写成</span></span><br><span class="line"><span class="type">Array</span>(<span class="string">&quot;hello you&quot;</span>,<span class="string">&quot;hello me&quot;</span>).flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br></pre></td></tr></table></figure></li><li><p><strong>foreach：迭代的意思</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//例如输出数组每个元素</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>).foreach(<span class="type">A</span>=&gt;println(<span class="type">A</span>))</span><br><span class="line"><span class="comment">//也可以简写成</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>).foreach(println(_))</span><br></pre></td></tr></table></figure></li><li><p><strong>filter：按照函数进行过滤操作</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//例如过滤出数组中的偶数</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).filter(<span class="type">A</span> =&gt; <span class="type">A</span> % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line"><span class="comment">//也可以简写成</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).filter(_ % <span class="number">2</span> == <span class="number">0</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>reduceLeft：按照函数从左往右的两两元素进行操作，例如累加、求最大值等</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//求数组的和</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>).reduceLeft((<span class="type">A</span>,<span class="type">B</span>)=&gt;<span class="type">A</span>+<span class="type">B</span>)</span><br><span class="line"><span class="comment">//也可以简写成</span></span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).reduceLeft(_+_)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Scala函数式编程特性一：函数赋值给变量&quot;&gt;&lt;a href=&quot;#Scala函数式编程特性一：函数赋值给变量&quot; class=&quot;headerlink&quot; title=&quot;Scala函数式编程特性一：函数赋值给变量&quot;&gt;&lt;/a&gt;Scala函数式编程特性一：函数赋值给变量&lt;/</summary>
      
    
    
    
    <category term="《7天极速掌握Scala》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8A7%E5%A4%A9%E6%9E%81%E9%80%9F%E6%8E%8C%E6%8F%A1Scala%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Scala" scheme="https://mactql.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Scala面向对象</title>
    <link href="https://mactql.github.io/posts/496349427.html"/>
    <id>https://mactql.github.io/posts/496349427.html</id>
    <published>2021-09-01T07:32:00.000Z</published>
    <updated>2021-09-02T07:26:26.134Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Scala的类和对象几乎和Java一样"><a href="#Scala的类和对象几乎和Java一样" class="headerlink" title="Scala的类和对象几乎和Java一样"></a>Scala的类和对象几乎和Java一样</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Point</span>(<span class="params">xc: <span class="type">Int</span>, yc: <span class="type">Int</span></span>) </span>&#123; <span class="comment">//构造函数是直接放在Class的参数列表里，这里和Java不同</span></span><br><span class="line">   <span class="keyword">var</span> x: <span class="type">Int</span> = xc</span><br><span class="line">   <span class="keyword">var</span> y: <span class="type">Int</span> = yc</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">move</span></span>(dx: <span class="type">Int</span>, dy: <span class="type">Int</span>) &#123;</span><br><span class="line">      x = x + dx</span><br><span class="line">      y = y + dy</span><br><span class="line">      println (<span class="string">&quot;x 的坐标点: &quot;</span> + x);</span><br><span class="line">      println (<span class="string">&quot;y 的坐标点: &quot;</span> + y);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p><strong>还有一种不需要类就可以创建对象的方法，直接用Object关键字，相当于Java的静态类</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span></span>&#123;</span><br><span class="line"><span class="keyword">var</span> age</span><br><span class="line">  ...</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line">使用时不需要<span class="keyword">new</span>，可以直接对象名.成员,如下</span><br><span class="line">println(<span class="type">Person</span>.age)</span><br></pre></td></tr></table></figure></li><li><p><strong>需要注意的是：</strong></p><ul><li><strong>如果object和class同名，他们为伴生关系，可以互相访问私有属性</strong></li><li><strong>可以在伴生object中创建一个apply函数，可以调用这个函数直接得到伴生class的对象实例</strong></li></ul></li></ul><hr><h2 id="Scala也必须要一个main方法才能运行"><a href="#Scala也必须要一个main方法才能运行" class="headerlink" title="Scala也必须要一个main方法才能运行"></a>Scala也必须要一个main方法才能运行</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">mainTest</span> </span>&#123;<span class="comment">//main方法只能定义在object中，不能在class中</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="Scala中的接口trait也比较特殊"><a href="#Scala中的接口trait也比较特殊" class="headerlink" title="Scala中的接口trait也比较特殊"></a>Scala中的接口trait也比较特殊</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">TraitA</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">A</span></span>(x: <span class="type">Int</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">TraitB</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">A</span></span>(x: <span class="type">Int</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">extends</span> <span class="title">TraitA</span> <span class="keyword">with</span> <span class="title">TraitB</span></span>&#123;<span class="comment">//实现用extends，多实现后面用with连接</span></span><br><span class="line"><span class="keyword">override</span> <span class="type">TraitA</span>(x:<span class="type">Int</span>)&#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">TraitB</span>(x:<span class="type">Int</span>)&#123;<span class="comment">//可以写override，也可以不写</span></span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Scala的类和对象几乎和Java一样&quot;&gt;&lt;a href=&quot;#Scala的类和对象几乎和Java一样&quot; class=&quot;headerlink&quot; title=&quot;Scala的类和对象几乎和Java一样&quot;&gt;&lt;/a&gt;Scala的类和对象几乎和Java一样&lt;/h2&gt;&lt;figu</summary>
      
    
    
    
    <category term="《7天极速掌握Scala》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8A7%E5%A4%A9%E6%9E%81%E9%80%9F%E6%8E%8C%E6%8F%A1Scala%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Scala" scheme="https://mactql.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Scala基础语法</title>
    <link href="https://mactql.github.io/posts/2823487899.html"/>
    <id>https://mactql.github.io/posts/2823487899.html</id>
    <published>2021-09-01T01:32:00.000Z</published>
    <updated>2021-09-02T07:26:22.353Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何申明变量和常量"><a href="#如何申明变量和常量" class="headerlink" title="如何申明变量和常量"></a>如何申明变量和常量</h2><ul><li><p><strong>val：常量</strong></p></li><li><p><strong>var：变量</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//数据类型如果不指定，会自动根据表达式来推断</span></span><br><span class="line"><span class="keyword">val</span> answer = <span class="number">0</span></span><br><span class="line"><span class="comment">//也可以指定数据类型</span></span><br><span class="line"><span class="keyword">val</span> answer: <span class="type">Int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="那么Scala有哪些数据类型呢？"><a href="#那么Scala有哪些数据类型呢？" class="headerlink" title="那么Scala有哪些数据类型呢？"></a>那么Scala有哪些数据类型呢？</h3></li><li><p><strong>基本数据类型：Byte,Char,Short,Int,Long,Float,Double,Boolean</strong></p></li><li><p><strong>增强版数据类型：StringOps、RichInt、RichChar、RichDouble等（比基本类型多了上百种功能）</strong></p><ul><li><strong>就以RichInt为例：<code>1.to(10)</code> 相当于Range 1 to 10</strong></li></ul></li></ul><hr><h2 id="Scala的-if-和Java是不同的"><a href="#Scala的-if-和Java是不同的" class="headerlink" title="Scala的 if 和Java是不同的"></a>Scala的 if 和Java是不同的</h2><p><strong>区别在于Scala的if是有返回值的，Java的if是没有的</strong><br><strong>举个例子：<code>val ret = if(18&gt;1) 1 else 0</code> 这里 ret 会接收 if 的返回值 1</strong></p><hr><h2 id="Scala的for和Java也不同，while相同"><a href="#Scala的for和Java也不同，while相同" class="headerlink" title="Scala的for和Java也不同，while相同"></a>Scala的for和Java也不同，while相同</h2><p><strong><code>for(i &lt;- 1 to/until n)</code> 意思就是 i 从1到n/n-1迭代</strong><br><strong>甚至可以迭代字符串中每个字符<code>for(c &lt;- &quot;string&quot;) println(c)</code></strong><br><strong>还有高级for循环：</strong></p><ul><li><strong>if守卫：<code>for(i &lt;- 1 to 10 if i % 2 == 0) println(i)</code> 就是把if判断写在for里面不满足就continue</strong></li><li><strong>yield：<code>for(i &lt;- 1 to 3) yield i * 2</code> 可以得到Vector(2，4，6) 就是在for循环中得到的数据组合成一个集合</strong></li></ul><hr><h2 id="Scala中的数组和Java类似"><a href="#Scala中的数组和Java类似" class="headerlink" title="Scala中的数组和Java类似"></a>Scala中的数组和Java类似</h2><ul><li><strong><code>val array = new Array[type](数组长度)</code></strong></li><li><strong>也可以这样写 <code>val array = Array(&quot;....&quot;,23,&quot;..&quot;)</code></strong></li><li><strong>甚至还有像ArrayList那样的可变长度的数组 <code>val ab = new ArrayBuffer[Int]()</code></strong></li><li><strong>还有一个元祖tuple也很常用，可存不同类型的数据 <code>val t = (..,...,...);</code> 并且可以用<code>t._i</code>来获取指定数据</strong></li></ul><hr><h2 id="Scala中也有集合Set、List、Map"><a href="#Scala中也有集合Set、List、Map" class="headerlink" title="Scala中也有集合Set、List、Map"></a>Scala中也有集合Set、List、Map</h2><ul><li><strong><code>val s = Set(1,2,3)</code>这样就直接创建了一个不可变的Set集合，还有HashSet、LinkedHashSet、SortedSet</strong></li><li><strong><code>val l = List(1,2,3)</code> 这样就直接创建了一个不可变的List集合</strong><ul><li><strong>List有很多方法，如<code>l.head</code>，<code>l.tail</code>，<code>for(i &lt;- l) println(i)</code></strong></li><li><strong>ListBuffer是一个可变型的List集合<code>val lb = scala.collection.mutable.ListBuffer[Int]()</code></strong></li></ul></li><li><strong><code>val m = Map((&quot;A&quot;,1),(&quot;B&quot;,2))</code> 这样就直接创建了一个不可变的Map集合</strong></li><li><strong>需要注意的是：</strong><ul><li><strong>如果创建的是一个可变型的集合，添加删除元素可以直接用 +=、-=</strong></li><li><strong>默认用Set、List、Map创建的都是不可变的集合，可以用<code>Scala.collection.mutable.Set/ListBuffer/Map[type](参数列表)</code></strong></li></ul></li></ul><hr><h2 id="Scala的函数和Java的方法也不一样"><a href="#Scala的函数和Java的方法也不一样" class="headerlink" title="Scala的函数和Java的方法也不一样"></a>Scala的函数和Java的方法也不一样</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//需要返回值的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span></span>(<span class="type">A</span>:<span class="class"><span class="keyword">type</span>,<span class="title">B</span></span>:<span class="class"><span class="keyword">type</span>) </span>= &#123;</span><br><span class="line">  ...</span><br><span class="line">  ...</span><br><span class="line">  <span class="type">A</span>,<span class="type">B</span> 最后一行就是返回值，不需要<span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//还有不需要返回值的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span></span>(<span class="type">A</span>:<span class="class"><span class="keyword">type</span>,<span class="title">B</span></span>:<span class="class"><span class="keyword">type</span>) </span>&#123; <span class="comment">//没有=号就是没有返回值</span></span><br><span class="line">  ...</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;如何申明变量和常量&quot;&gt;&lt;a href=&quot;#如何申明变量和常量&quot; class=&quot;headerlink&quot; title=&quot;如何申明变量和常量&quot;&gt;&lt;/a&gt;如何申明变量和常量&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;val：常量&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;</summary>
      
    
    
    
    <category term="《7天极速掌握Scala》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8A7%E5%A4%A9%E6%9E%81%E9%80%9F%E6%8E%8C%E6%8F%A1Scala%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Scala" scheme="https://mactql.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>初识Spark与工作原理</title>
    <link href="https://mactql.github.io/posts/2693573151.html"/>
    <id>https://mactql.github.io/posts/2693573151.html</id>
    <published>2021-08-30T01:43:00.000Z</published>
    <updated>2021-09-09T13:44:48.984Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是Spark？"><a href="#什么是Spark？" class="headerlink" title="什么是Spark？"></a>什么是Spark？</h2><p><strong>Spark是用来替换MapReduce的，因为它基于<code>内存计算</code>，可以比MapReduce快几十倍几百倍</strong></p><hr><h2 id="Spark怎么和Hadoop结合使用？"><a href="#Spark怎么和Hadoop结合使用？" class="headerlink" title="Spark怎么和Hadoop结合使用？"></a>Spark怎么和Hadoop结合使用？</h2><p><strong>如下图所示，后面几章将着重介绍Spark Core和Spark SQL</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86Spark%E4%B8%8E%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="这里简单介绍Spark的工作原理，后面章节会深入分析"><a href="#这里简单介绍Spark的工作原理，后面章节会深入分析" class="headerlink" title="这里简单介绍Spark的工作原理，后面章节会深入分析"></a>这里简单介绍Spark的工作原理，后面章节会深入分析</h2><p><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86Spark%E4%B8%8E%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><blockquote><p><strong>从简化的工作原理图可以看到：</strong></p><ol><li><strong>HDFS的数据加载到内存中转化为RDD，然后假设RDD被分成三份，分别放到三个节点上去，这样可以并行处理</strong></li><li><strong>中间可以调用多个高阶函数处理后形成新的RDD，然后再通过Map或者其他的一些高阶函数进行处理</strong></li><li><strong>然后把数据存储出去</strong></li></ol></blockquote><hr><h2 id="什么是RDD？"><a href="#什么是RDD？" class="headerlink" title="什么是RDD？"></a>什么是RDD？</h2><blockquote><ul><li><strong>RDD其实是一个抽象概念，弹性分布式数据集。</strong></li><li><strong>RDD这个数据集一般情况是放在内存里的。并且RDD可以分区，每个分区放在集群的不同节点上，从而可以并行操作。另外如果某个节点故障导致那个RDD分区的数据丢了，RDD会自动重新计算这个分区的数据</strong></li><li><strong>通俗来说，RDD的表现形式类似数据库的视图，是抽象的，如下图所示</strong></li></ul></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86Spark%E4%B8%8E%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/2.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="下面简单介绍一下Spark架构原理"><a href="#下面简单介绍一下Spark架构原理" class="headerlink" title="下面简单介绍一下Spark架构原理"></a>下面简单介绍一下Spark架构原理</h2><p><strong>[这里以Spark的standalone集群进行分析]</strong></p><blockquote><p><strong>首先要了解几个概念：</strong></p><ul><li><strong>Drive 进程：就是负责执行我们自己编写的 Spark 程序，他所在的节点可以由我们指定，可以是集群上的任意节点</strong></li><li><strong>Master 进程：集群中主节点启动的进程，负责集群资源管理和监控</strong></li><li><strong>Worker 进程：集群中从节点启动的进程，负责启动其他进程( Executor 进程)来执行任务</strong></li><li><strong>Executor 进程：就是 Worker 进程启动的进程</strong></li><li><strong>Task 线程：由 Executor 进程启动的 线程 ，具体运行的就是一些高阶函数、Map操作</strong></li><li><strong>反注册：Executor 进程告诉 Drive 进程，使得 Drive 进程知道有哪些Executor，方便提交task给他们</strong></li></ul></blockquote><p><strong>​</strong></p><blockquote><p><strong>接下来学习Spark架构原理，如下图所示：(仅作了解即可，之后深入分析)</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86Spark%E4%B8%8E%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/3.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是Spark？&quot;&gt;&lt;a href=&quot;#什么是Spark？&quot; class=&quot;headerlink&quot; title=&quot;什么是Spark？&quot;&gt;&lt;/a&gt;什么是Spark？&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Spark是用来替换MapReduce的，因为它基于&lt;code&gt;内存</summary>
      
    
    
    
    <category term="《Spark快速上手》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8ASpark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>第7章 MapReduce工作机制</title>
    <link href="https://mactql.github.io/posts/2408565548.html"/>
    <id>https://mactql.github.io/posts/2408565548.html</id>
    <published>2021-08-27T12:58:00.000Z</published>
    <updated>2021-08-27T12:59:37.971Z</updated>
    
    <content type="html"><![CDATA[<p><strong>在前面章节，我们简单了解了YARN的工作机制，这一章将详细介绍介绍MapReduce是怎么运行</strong></p><h2 id="MapReduce-YARN的工作机制？"><a href="#MapReduce-YARN的工作机制？" class="headerlink" title="MapReduce YARN的工作机制？"></a>MapReduce YARN的工作机制？</h2><p><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC7%E7%AB%A0MapReduce%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><blockquote><p><strong>结合上图来学习一下每个步骤：</strong></p><ol><li><strong>首先是提交作业，直接调用 Job 对象的 submit() 即可，他会创建一个 JobSummiter 实例</strong></li><li><strong>然后这个 JoSummiter 实例会申请 ResourceManager 给这个作业一个 ID ，ResourceManager 会检查一下</strong></li><li><strong>然后把需要的资源复制到 HDFS 的文件名是 ID 号的目录下</strong></li><li><strong>然后 ResourceManager 调用 submitApplication() 方法提交作业</strong></li><li><strong>ResourceManager 分配第一个容器 Container；然后 ResourceManager 在 NodeManager 的管理下，在容器中运行作业的 AppMaster 进程</strong></li><li><strong>AppMaster 对作业进行初始化</strong></li><li><strong>然后 AppMaster 接收输入数据分片，为每个分片创建一个 Map 任务对象</strong></li><li><strong>如果是小作业，就直接在自己节点的JVM中运行。大作业的话就向 ResourceManager 申请 Map 任务和 Reduce 任务的容器 Container</strong></li><li><strong>分配好容器后，AppMaster 就会和这些容器所在节点的 nodemanager 通信来启动这些容器</strong></li><li><strong>在运行 task 之前，还要接收数据放到自己的本地磁盘上</strong></li><li><strong>最后运行 Map 任务或者 Reduce 任务</strong></li></ol></blockquote><hr><h2 id="其实在Map和Reduce之间还有一层shuffle"><a href="#其实在Map和Reduce之间还有一层shuffle" class="headerlink" title="其实在Map和Reduce之间还有一层shuffle"></a>其实在Map和Reduce之间还有一层shuffle</h2><blockquote><p><strong>什么叫shuffle？</strong></p><ul><li><strong>在 Reduce 操作之前，需要保证数据过来都是按照 Key 排序的</strong></li><li><strong>而把 Map 输出的数据按照 Key 排序并输入给 Reduce 的过程就叫 Shuffle</strong></li></ul></blockquote><p><strong>那么是怎么进行shuffle的呢？</strong></p><blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC7%E7%AB%A0MapReduce%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"><br><strong>如图所示，绿色的虚线框框就表示的从 Map 输出到 Reduce 输入中间的 Shuffle过程</strong><br><strong>并且中间两个红色虚线框框分别是 Map 端的 shuffle 和 Reduce 端的 shuffle</strong></p></blockquote><blockquote><p><strong>首先是 Map 端的Shuffle</strong></p><ol><li><strong>每个数据分片由Map任务执行完后，不是直接存到磁盘的，会先存到一个环形缓冲区里</strong></li><li><strong>当这个环形缓冲区快存不下了，就会溢出到磁盘上，形成一个一个溢出文件，这里有些细节需要<code>非常注意</code></strong><ol><li><strong>首先，在往磁盘溢出的过程中，会先按照 Reduce 的个数进行分区，目的是用来指定这个数据要交给哪个 Reducer 处理，就像图中 第2步 有红黄两个 Reducer ，就分两个区，把溢出的文件放到相应的分区里</strong></li><li><strong>另外，在每个分区中还会根据 Key 进行排序，来一个排一个</strong></li><li><strong>如果为了优化性能，设置了 Combiner ，也就是提前 Reduce ，也是在分区内进行操作</strong></li></ol></li><li><strong>随着溢出文件越来越多，在Map任务结束之前会把所有溢出文件合并成一个已分区和排序的输出文件，如图中的第3步</strong></li></ol></blockquote><blockquote><p><strong>还有Reduce端的Shuffle</strong></p><ol><li><strong>每个Map完成时间不同，所以一旦某个Map完成，Reduce都会开始复制Map的输出。每个Reducer都会复制对应分区的数据</strong></li><li><strong>如果数据比较少，就放到内存缓冲区，如果放不下了就溢出到磁盘上，和Map端一样不断合并和排序，最后合并成一个按Key排序的文件作为Reduce输入</strong></li></ol></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;在前面章节，我们简单了解了YARN的工作机制，这一章将详细介绍介绍MapReduce是怎么运行&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;MapReduce-YARN的工作机制？&quot;&gt;&lt;a href=&quot;#MapReduce-YARN的工作机制？&quot; class=</summary>
      
    
    
    
    <category term="《Hadoop权威指南》读书笔记" scheme="https://mactql.github.io/categories/%E3%80%8AHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MapReduce" scheme="https://mactql.github.io/tags/MapReduce/"/>
    
    <category term="YARN" scheme="https://mactql.github.io/tags/YARN/"/>
    
  </entry>
  
  <entry>
    <title>第4章 关于YARN</title>
    <link href="https://mactql.github.io/posts/1038205382.html"/>
    <id>https://mactql.github.io/posts/1038205382.html</id>
    <published>2021-08-26T13:58:00.000Z</published>
    <updated>2021-08-27T06:33:45.964Z</updated>
    
    <content type="html"><![CDATA[<h2 id="首先YARN是什么？"><a href="#首先YARN是什么？" class="headerlink" title="首先YARN是什么？"></a>首先YARN是什么？</h2><blockquote><p><strong>在Hadoop1.0的时候，MapReduce的JobTracker负责了太多工作，接收任务是它，资源调度是它，监控TaskTracker还是它，显然不合理</strong><br><strong>所以在hadoop2.0的时候就把资源调度的任务分离出来，让YARN接手这个任务</strong><br><strong>所以YARN就是一个资源调度框架</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC4%E7%AB%A0%E5%85%B3%E4%BA%8EYARN/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p></blockquote><hr><h2 id="YARN是怎么运行的呢？"><a href="#YARN是怎么运行的呢？" class="headerlink" title="YARN是怎么运行的呢？"></a>YARN是怎么运行的呢？</h2><blockquote><p><strong>在介绍YARN运行机制之前，首先要了解几个概念：</strong></p><ul><li><strong>Container：容器，是YARN对资源进行的一层抽象，如把CPU核数、内存等计算资源封装成一个个Container</strong></li><li><strong>ResourceManager：负责资源调度，整个系统只有一个ResourceManager，例如调度刚刚学的Container</strong></li><li><strong>NodeManager：是ResourceManager在每台机器上的代理，负责管理和监控Container</strong></li><li><strong>ApplicationMaster：负责协调运行MapReduce作业，把一个作业拆成多个 task，并向 ResourceManager 申请容器</strong></li></ul></blockquote><h3 id="我们来看一下提交一个作业到YARN中的流程："><a href="#我们来看一下提交一个作业到YARN中的流程：" class="headerlink" title="我们来看一下提交一个作业到YARN中的流程："></a>我们来看一下提交一个作业到YARN中的流程：</h3><p><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC4%E7%AB%A0%E5%85%B3%E4%BA%8EYARN/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><blockquote><p><strong>我们来具体说说每一步的过程：</strong></p><ol><li><strong>首先 Client 向 Yarn 提交 Application，假设是一个 MapReduce 作业</strong></li><li><strong>然后 ResourceManager 向 NodeManager 申请第一个容器，在这个容器里运行对应的 ApplicationMaster 进程</strong></li><li><strong>然后 ApplicationMaster 把这个作业拆成多个 task ，这些 task 可以在一个或多个容器里运行</strong></li><li><strong>然后向 ResourceManager 申请要运行程序的容器，并定时发送心跳</strong></li><li><strong>申请到容器后，ApplicationMaster 把作业发到对应容器的多个 NodeManager 的容器里去运行，这里运行的可能是 Map 任务，也可能是 Reduce 任务</strong></li><li><strong>运行任务的时候会向 ApplicationMaster 发送心跳，汇报情况</strong></li><li><strong>程序运行完成后， ApplicationMaster 再向 ResourceManager 注销并释放容器资源</strong></li></ol></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;首先YARN是什么？&quot;&gt;&lt;a href=&quot;#首先YARN是什么？&quot; class=&quot;headerlink&quot; title=&quot;首先YARN是什么？&quot;&gt;&lt;/a&gt;首先YARN是什么？&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;在Hadoop1.0的时候，Map</summary>
      
    
    
    
    <category term="《Hadoop权威指南》读书笔记" scheme="https://mactql.github.io/categories/%E3%80%8AHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="YARN" scheme="https://mactql.github.io/tags/YARN/"/>
    
  </entry>
  
  <entry>
    <title>第3章 关于HDFS</title>
    <link href="https://mactql.github.io/posts/86411073.html"/>
    <id>https://mactql.github.io/posts/86411073.html</id>
    <published>2021-08-26T07:22:00.000Z</published>
    <updated>2021-08-26T07:39:06.097Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么要用HDFS，优点和缺点是什么？"><a href="#为什么要用HDFS，优点和缺点是什么？" class="headerlink" title="为什么要用HDFS，优点和缺点是什么？"></a>为什么要用HDFS，优点和缺点是什么？</h2><blockquote><p><strong>首先说一下优点：</strong></p><ul><li><strong>可以存超大文件</strong></li><li><strong>一次写入，多次读取</strong></li><li><strong>可运行在廉价集群上，一个节点坏了还能继续运行</strong></li></ul></blockquote><blockquote><p><strong>然后是缺点：</strong></p><ul><li><strong>不能低延迟时间的访问：HDFS是为了高吞吐优化的，如果要低延迟可以用HBase</strong></li><li><strong>不能有大量小文件：因为namenode把文件的元数据都存在内存，大量小文件会给namenode巨大压力</strong></li><li><strong>不能多用户进行写入操作，也不能任意位置修改文件</strong></li></ul></blockquote><hr><h2 id="下面开始介绍HDFS"><a href="#下面开始介绍HDFS" class="headerlink" title="下面开始介绍HDFS"></a>下面开始介绍HDFS</h2><h3 id="首先要了解HDFS数据块"><a href="#首先要了解HDFS数据块" class="headerlink" title="首先要了解HDFS数据块"></a>首先要了解HDFS数据块</h3><blockquote><ul><li><strong>磁盘有一个Block的概念，它是磁盘读写数据的最小单位，HDFS也有Block</strong></li><li><strong>HDFS可以把一个大文件按照Block的大小进行拆解，存到不同的Block上，并且所有的Block不需要在同一个节点上</strong></li><li><strong>每个Block都可以单独做备份，防止节点坏掉数据丢失</strong></li></ul></blockquote><h3 id="然后要了解的是Namenode和Datanode"><a href="#然后要了解的是Namenode和Datanode" class="headerlink" title="然后要了解的是Namenode和Datanode"></a>然后要了解的是Namenode和Datanode</h3><blockquote><ul><li><strong>Namenode是管理员：管理整个文件系统的数结构和元数据，包括HDFS系统快照和日志文件，并且他知道一个文件的全部Block在哪些Datanode上</strong></li><li><strong>Datanode是工作者：他就负责存储和读取这些Block，还会定期向Namenode汇报他存储的Block的列表</strong></li></ul></blockquote><h3 id="最后是最重要的HDFS是怎么读写的？"><a href="#最后是最重要的HDFS是怎么读写的？" class="headerlink" title="最后是最重要的HDFS是怎么读写的？"></a>最后是最重要的HDFS是怎么读写的？</h3><blockquote><p><strong>首先是从读取数据：</strong></p><ol><li><strong>首先client调用DistributedFS的一个实例的open()方法</strong></li><li><strong>然后DistributedFS会调用Namenode获取文件起始块的Datanode地址</strong></li><li><strong>这个时候DistributedFS会返回一个FSDataInputStream的数据流对象</strong></li><li><strong>然后可以反复调用这个对象的read()方法，数据从Datanode传输到客户端</strong></li><li><strong>读取过程中FSDataInputStream会封装一个DFSInputStream去问Namenode下一批数据块的Datanode地址</strong></li><li><strong>读完就close()</strong></li></ol></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC3%E7%AB%A0%E5%85%B3%E4%BA%8EHDFS/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><blockquote><p><strong>然后是写入数据：</strong></p><ol><li><strong>首先client调用DistributedFS实例的create()方法来新建文件</strong></li><li><strong>然后DistributedFS就会请求Namenode创建一个文件，namenode经过一系列检查后创建一条新建记录</strong></li><li><strong>然后DistributedFS向client返回一个FSDataOutputStream对象，这样就可以开始写数据了</strong></li><li><strong>写的过程中，FSDataOutputStream会封装一个DFSOutputStream对象，这个对象会把数据拆成一个个数据包，放到”数据包队列中”</strong></li><li><strong>然后DataStreamer会让一组Datanode分配新的Block来存储这些数据，然后把数据一个个发送到这些Datanode组成的管道</strong></li><li><strong>管道中所有Datanode都收到后会发送ack应答包回来，然后只有收到ack后数据包才会从队列中删除</strong></li><li><strong>写完后close()</strong></li></ol></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC3%E7%AB%A0%E5%85%B3%E4%BA%8EHDFS/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;为什么要用HDFS，优点和缺点是什么？&quot;&gt;&lt;a href=&quot;#为什么要用HDFS，优点和缺点是什么？&quot; class=&quot;headerlink&quot; title=&quot;为什么要用HDFS，优点和缺点是什么？&quot;&gt;&lt;/a&gt;为什么要用HDFS，优点和缺点是什么？&lt;/h2&gt;&lt;bloc</summary>
      
    
    
    
    <category term="《Hadoop权威指南》读书笔记" scheme="https://mactql.github.io/categories/%E3%80%8AHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="HDFS" scheme="https://mactql.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>第2章 关于MapReduce</title>
    <link href="https://mactql.github.io/posts/3366258919.html"/>
    <id>https://mactql.github.io/posts/3366258919.html</id>
    <published>2021-08-25T08:18:00.000Z</published>
    <updated>2021-08-25T08:19:15.795Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Map和Reduce分别是什么？"><a href="#Map和Reduce分别是什么？" class="headerlink" title="Map和Reduce分别是什么？"></a>Map和Reduce分别是什么？</h2><blockquote><ul><li><strong>MapReduce任务过程分为两个阶段，分别是Map和Reduce，即程序员实现Mapper和Reducer两个接口</strong></li><li><strong>什么是Map，就是拆，把拼好的乐高汽车玩具拆成一块块积木，每个积木都是一个<code>&lt;Key,Value&gt;</code>键值对</strong></li><li><strong>在Map和Reduce中间有时候需要shuffle，就是把Map拆出来的&lt;Key,Value&gt;打乱，并按照key排序和分组，组成<code>&lt;Key,List&lt;Value&gt;&gt;</code></strong></li><li><strong>什么是Reduce，就是组合，用积木组合成变形金刚，对Value进行一些操作，如SUM、MAX等，得到一个<code>&lt;Key,Value&gt;</code></strong></li></ul></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC2%E7%AB%A0%E5%85%B3%E4%BA%8EMapReduce/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="MapReduce的数据流是什么样的？"><a href="#MapReduce的数据流是什么样的？" class="headerlink" title="MapReduce的数据流是什么样的？"></a>MapReduce的数据流是什么样的？</h2><ul><li><strong>首先对于一个MapReduce作业(job)，Hadoop会分为Map任务和Reduce任务</strong></li><li><strong>Hadoop会把输入数据分成多个大小相同的数据块，即<code>数据分片</code>，每个分片对应一个Map任务，并且由这个Map任务来运行自定义的Map函数</strong></li></ul><p><strong>需要注意的是：</strong></p><ul><li><strong>在创建Map任务时，一般会在对应分片的机器上运行，否则再考虑同一个机架上的其他机器，甚至是其他机架</strong></li><li><strong>Map任务会输出在<code>本地磁盘</code>上，而不是HDFS。因为数据是暂时的，作业完成就可以删除</strong></li><li><strong>可以使用Combiner函数<code>优化</code>MapReduce，例如求MAX，可以在每个分片Map任务后先求MAX，然后再传给Reduce，这样就减少了Map和Reduce之间的传输数据量，提高性能</strong></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Map和Reduce分别是什么？&quot;&gt;&lt;a href=&quot;#Map和Reduce分别是什么？&quot; class=&quot;headerlink&quot; title=&quot;Map和Reduce分别是什么？&quot;&gt;&lt;/a&gt;Map和Reduce分别是什么？&lt;/h2&gt;&lt;blockquote&gt;
&lt;ul&gt;</summary>
      
    
    
    
    <category term="《Hadoop权威指南》读书笔记" scheme="https://mactql.github.io/categories/%E3%80%8AHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MapReduce" scheme="https://mactql.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>Spark实现基于GA的BN结构学习</title>
    <link href="https://mactql.github.io/posts/3842144158.html"/>
    <id>https://mactql.github.io/posts/3842144158.html</id>
    <published>2021-08-25T06:07:00.000Z</published>
    <updated>2021-08-25T06:10:59.901Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本笔记是对《贝叶斯网络结构学习算法研究》论文进行学习时所写</strong><br><strong>上篇文章我们了解了什么是HDFS、MapReduce和Spark</strong><br><strong>该论文还需要学习的内容如下：（本文只解决第一个问题）</strong></p><ul><li><strong>如何用分布式实现GA算法？</strong></li><li><strong>如何用Spark实现基于GA的BN结构学习算法？</strong><ul><li><strong>如何基于Spark构造超结构？</strong></li><li><strong>如何基于Spark实现评分计算？</strong></li><li><strong>如何基于Spark实现种群演化？</strong></li></ul></li></ul><hr><h2 id="第1个问题，如何用分布式实现GA算法？"><a href="#第1个问题，如何用分布式实现GA算法？" class="headerlink" title="第1个问题，如何用分布式实现GA算法？"></a>第1个问题，如何用分布式实现GA算法？</h2><blockquote><p><strong>使用分布式实现GA算法的方法有三类，主从模型、岛屿模型和网格模型，该论文使用的是主从模型，故仅对此进行介绍</strong></p><ul><li><strong>主从模型：用一个主节点来管理所有的子种群，并且个体放在从节点上，然后分别在从节点上计算个体的适应度</strong></li></ul></blockquote><p><strong>流程呢就是：基于主从模型，用MapReduce跑GA的评价算子(适应度函数)</strong></p><blockquote><ul><li><strong>可以用多个Mapper并行计算每个染色体的适应度，然后用单个Reducer收集结果和完成遗传操作(选择、交叉、变异)，这样繁殖一代就是一轮MapReduce</strong></li></ul></blockquote><hr><h2 id="第2个问题，如何用Spark实现基于GA的BN结构学习算法？"><a href="#第2个问题，如何用Spark实现基于GA的BN结构学习算法？" class="headerlink" title="第2个问题，如何用Spark实现基于GA的BN结构学习算法？"></a>第2个问题，如何用Spark实现基于GA的BN结构学习算法？</h2><blockquote><p><strong>首先再次说一下如何基于GA实现BN结构学习算法？</strong></p><ol><li><strong>首先要构造一个超结构</strong></li><li><strong>初始化GA种群</strong></li><li><strong>然后对这一代进行适应度计算，即BIC评分</strong></li><li><strong>然后开始交叉变异，繁衍下一代</strong></li></ol><ul><li><strong>重复上述操作，直到满足条件</strong></li></ul></blockquote><blockquote><p><strong>那么怎么用Spark实现基于GA的BN结构学习算法呢？</strong><br><img "" class="lazyload placeholder" data-original="/medias/Spark%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EGA%E7%9A%84BN%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ol><li><strong>首先用Spark并行计算互信息并构建超结构，初始化种群</strong></li><li><strong>然后用Spark并行计算个体的BIC评分</strong></li><li><strong>然后用Spark并行演化出下一代个体</strong></li></ol><ul><li><strong>重复上述操作，直到满足条件，最后选择BIC评分最高的个体作为最终BN结构</strong></li></ul></blockquote><blockquote><p><strong>故下面将详细介绍上述三个操作的步骤</strong></p></blockquote><hr><h2 id="基于Spark实现的第1个操作，如何构造超结构？"><a href="#基于Spark实现的第1个操作，如何构造超结构？" class="headerlink" title="基于Spark实现的第1个操作，如何构造超结构？"></a>基于Spark实现的第1个操作，如何构造超结构？</h2><h3 id="首先搞清楚啥叫超结构？"><a href="#首先搞清楚啥叫超结构？" class="headerlink" title="首先搞清楚啥叫超结构？"></a>首先搞清楚啥叫超结构？</h3><blockquote><ul><li><strong>如下图，G的每条边都在S上，所以S是G的超结构，这种就叫超结构。但是不是G’的超结构因为W-&gt;M不在S上</strong></li></ul></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/Spark%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EGA%E7%9A%84BN%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><h2 id="如何构造超结构？"><a href="#如何构造超结构？" class="headerlink" title="如何构造超结构？"></a>如何构造超结构？</h2><blockquote><ol><li><strong>首先从HDFS中获取数据集D</strong></li><li><strong>计算互信息所需要的”情况变量集”，这个情况变量集就是放数据集中所有的情况，然后标上序号，给Mmi使用</strong></li></ol></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/Spark%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EGA%E7%9A%84BN%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0/2.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><blockquote><ol start="3"><li><strong>然后广播这个”情况变量集”</strong></li><li><strong>如下图所示，就是对数据集统计各情况出现的次数，得到键值对存入Redis。用这些次数构建一个辅助矩阵Mmi，我猜这个矩阵应该是一个一维数组，放了情况变量集的每种情况在数据集中出现的次数，这个数组可在BIC计算时复用</strong></li></ol></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/Spark%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EGA%E7%9A%84BN%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0/4.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><blockquote><ol start="5"><li><strong>然后通过辅助矩阵Mmi并通过互信息计算公式如下图计算两两节点的互信息，构造互信息矩阵</strong><img "" class="lazyload placeholder" data-original="/medias/Spark%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EGA%E7%9A%84BN%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0/5.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></li><li><strong>互信息值记为MI，最后步骤如下图所示</strong></li></ol></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/Spark%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EGA%E7%9A%84BN%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0/6.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="基于Spark实现的第2个操作，如何初始化GA种群？"><a href="#基于Spark实现的第2个操作，如何初始化GA种群？" class="headerlink" title="基于Spark实现的第2个操作，如何初始化GA种群？"></a>基于Spark实现的第2个操作，如何初始化GA种群？</h2><p><strong>在我们构建完超结构后，还需要初始化种群RDD</strong></p><ol><li><strong>首先要把超结构广播到每个分区</strong></li><li><strong>然后在每个分区，用Map方法基于超结构进行随机生成个体，例如超结构中每个边的两点AB，随机生成A-&gt;B、B-&gt;A和无连接，这样得到的个体组成BN结构种群RDD</strong></li><li><strong>但是BN结构不能有环，使用GR去环算法，得到合法的种群RDD</strong></li></ol><p><img "" class="lazyload placeholder" data-original="/medias/Spark%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EGA%E7%9A%84BN%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0/7.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="基于Spark实现的第3个操作，如何实现评分计算？"><a href="#基于Spark实现的第3个操作，如何实现评分计算？" class="headerlink" title="基于Spark实现的第3个操作，如何实现评分计算？"></a>基于Spark实现的第3个操作，如何实现评分计算？</h2><ol><li><strong>首先对种群RDD中每个BN结构BIC计算时需要的情况进行计算，然后把每个分区计算的这个集合合并</strong></li><li><strong>然后把redis中已经算过的去掉，剩下的情况集合记为Sneeds广播</strong></li><li><strong>还是用Map方法对RDD中的每个分区计算Sneeds中情况出现的次数，构造情况出现次数键值对</strong></li><li><strong>Reduce方法在每个分区求和，并计入Redis</strong></li><li><strong>还是用Map对分区内每个个体计算BIC评分，还是构造每个个体的评分键值对</strong></li></ol><p><img "" class="lazyload placeholder" data-original="/medias/Spark%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EGA%E7%9A%84BN%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0/8.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="基于Spark实现的第4个操作，如何并行演化？"><a href="#基于Spark实现的第4个操作，如何并行演化？" class="headerlink" title="基于Spark实现的第4个操作，如何并行演化？"></a>基于Spark实现的第4个操作，如何并行演化？</h2><ul><li><strong>首先要进行选择和交叉：</strong><ol><li><strong>首先广播种群数组，记种群数组长度为Nc</strong></li><li><strong>然后构建长度为分区数N的锦标赛数组，其中每个元素是长度为T的[0,Nc)的随机数的数组TSI，广播这个数组</strong></li><li><strong>然后在每个分区中再随机添加两个TSI数组，这样每个分区都有三个TSI数组</strong></li><li><strong>每个分区用Map方法，对第一个TSI数组直接取得BN个体，记为G0；后两个TSI分别得到两个&lt;BN,BIC评分&gt;的键值对，进行均匀交叉，并对交叉后的BN个体通过去环算法合法化，记为G1</strong></li><li><strong>然后对交叉后的BN个体G1进行单点突变，并通过去环算法合法化，记为G2</strong></li><li><strong>最后把G0和G2作为数组输出，作为下一代种群RDD</strong></li></ol></li></ul><p><img "" class="lazyload placeholder" data-original="/medias/Spark%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EGA%E7%9A%84BN%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0/9.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;本笔记是对《贝叶斯网络结构学习算法研究》论文进行学习时所写&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;上篇文章我们了解了什么是HDFS、MapReduce和Spark&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;该论文还需要学习的内容如下：（本文只解决第一个问题）</summary>
      
    
    
    
    <category term="《贝叶斯网络结构学习算法》论文笔记" scheme="https://mactql.github.io/categories/%E3%80%8A%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="贝叶斯网络" scheme="https://mactql.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/"/>
    
    <category term="遗传算法" scheme="https://mactql.github.io/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/"/>
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>初识Spark及HDFS与MR</title>
    <link href="https://mactql.github.io/posts/367393044.html"/>
    <id>https://mactql.github.io/posts/367393044.html</id>
    <published>2021-08-19T07:54:00.000Z</published>
    <updated>2021-08-25T08:16:17.659Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本笔记是对《贝叶斯网络结构学习算法研究》论文进行学习时所写</strong><br><strong>上篇文章我们了解了什么是BN结构学习算法以及GA算法</strong><br><strong>该论文还需要学习的内容如下：（本文只解决第一个问题）</strong></p><ul><li><strong>什么是Spark分布式计算平台？</strong></li><li><strong>如何用分布式实现GA算法？</strong></li><li><strong>如何并行化构造超结构？</strong></li><li><strong>如何基于Spark实现GA算法？</strong></li><li><strong>如何基于Spark实现评分计算？</strong></li></ul><hr><h2 id="在学习Spark之前，先了解以下分布式计算框架"><a href="#在学习Spark之前，先了解以下分布式计算框架" class="headerlink" title="在学习Spark之前，先了解以下分布式计算框架"></a>在学习Spark之前，先了解以下分布式计算框架</h2><blockquote><p><strong>首先是Hadoop平台，即一个面向大数据的分布式基础架构</strong><br><strong>其中HDFS是分布式文件系统，把大数据进行切分然后存储到HDFS的若干节点上</strong></p><ul><li><strong>HDFS有两种节点，NameNode(NN)和DataNode(DN)。运行时一般是一个NN多个DN</strong></li></ul></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86Spark%E5%8F%8AHDFS%E4%B8%8EMR/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><blockquote><ul><li><strong>读取步骤：首先HDFS客户端通过分布式文件系统向NN请求下载文件，NN查询元数据找到该文件所在的多个DN地址，客户端依次通过FSData输入流向DN地址请求并读取数据，直到完成读取</strong></li></ul></blockquote><blockquote><p><strong>然后介绍一下MapReduce(MR)，MR是一个并行处理数据的编程模型，用来对大数据进行计算</strong></p><ul><li><strong>MR流程分为两个阶段，Map阶段和Reduce阶段，分别由Mapper和Reducer两个接口实现</strong></li><li><strong>什么是Map，就是拆，把拼好的乐高汽车玩具拆成一块块积木</strong></li><li><strong>什么是Reduce，就是组合，用积木组合成变形金刚</strong></li><li><strong>那么怎么实现MR呢？如下图所示，一般先把大数据分成一”片”一”片”，然后每一”片”都由一个Map去拆，拆好了以后再shuffle归类成一组一组的，然后Reduce把每一组进行组合</strong></li></ul></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86Spark%E5%8F%8AHDFS%E4%B8%8EMR/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><h2 id="终于可以开始学习Spark了"><a href="#终于可以开始学习Spark了" class="headerlink" title="终于可以开始学习Spark了"></a>终于可以开始学习Spark了</h2><blockquote><p><strong>Spark是基于MR的一个处理大数据的计算框架，比MR速度更快</strong><br><strong>其中SparkSQL用来处理数据，SparkML用来做机器学习，SparkStreaming用来做流计算</strong><br><strong>Spark底层是用的RDD来处理数据的，RDD是什么？</strong></p><ul><li><strong>RDD其实是一个抽象概念，弹性分布式数据集。</strong></li><li><strong>RDD这个数据集是放在内存里的，并且RDD可以分区，每个分区放在集群的不同节点上，从而可以并行操作</strong></li><li><strong>通俗来说，RDD的表现形式类似数据库的视图，是抽象的，如下图所示</strong></li></ul></blockquote><p><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86Spark%E5%8F%8AHDFS%E4%B8%8EMR/2.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;本笔记是对《贝叶斯网络结构学习算法研究》论文进行学习时所写&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;上篇文章我们了解了什么是BN结构学习算法以及GA算法&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;该论文还需要学习的内容如下：（本文只解决第一个问题）&lt;/stro</summary>
      
    
    
    
    <category term="《贝叶斯网络结构学习算法》论文笔记" scheme="https://mactql.github.io/categories/%E3%80%8A%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Spark" scheme="https://mactql.github.io/tags/Spark/"/>
    
    <category term="HDFS" scheme="https://mactql.github.io/tags/HDFS/"/>
    
    <category term="MapReduce" scheme="https://mactql.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>初识贝叶斯网络与遗传算法</title>
    <link href="https://mactql.github.io/posts/1117388134.html"/>
    <id>https://mactql.github.io/posts/1117388134.html</id>
    <published>2021-08-18T07:21:00.000Z</published>
    <updated>2021-08-19T07:52:17.756Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本笔记是对《贝叶斯网络结构学习算法研究》论文进行学习时所写</strong></p><h2 id="首先通过摘要"><a href="#首先通过摘要" class="headerlink" title="首先通过摘要"></a>首先通过摘要</h2><p><strong>可以了解到该论文研究的方向，即在大数据情况下基于遗传算法的贝叶斯网络结构算法执行效率问题</strong><br><strong>作者将混合方式的 BN 结构学习算法与 Spark 分布式计算平台结合</strong><br><strong>在构建超结构、评分计算和 GA 的进化操作三个流程上进行了并行化工作</strong><br><strong>同时引入Redis对中间数据进行存储以便在评分计算过程中高效复用数据</strong></p><hr><h2 id="故对该论文需要学习的内容如下：（本文只解决前3个问题）"><a href="#故对该论文需要学习的内容如下：（本文只解决前3个问题）" class="headerlink" title="故对该论文需要学习的内容如下：（本文只解决前3个问题）"></a>故对该论文需要学习的内容如下：（本文只解决前3个问题）</h2><ul><li><strong>什么是BN结构学习算法？</strong></li><li><strong>什么是GA算法？</strong></li><li><strong>怎么使用GA算法来得到BN结构？</strong></li><li><strong>什么是Spark分布式计算平台？</strong></li><li><strong>如何并行化构造超结构？</strong></li><li><strong>如何基于Spark实现GA算法？</strong></li><li><strong>如何基于Spark实现评分计算？</strong></li></ul><hr><h2 id="第1个问题，什么是BN结构学习算法？"><a href="#第1个问题，什么是BN结构学习算法？" class="headerlink" title="第1个问题，什么是BN结构学习算法？"></a>第1个问题，什么是BN结构学习算法？</h2><h3 id="首先回答什么是BN结构？"><a href="#首先回答什么是BN结构？" class="headerlink" title="首先回答什么是BN结构？"></a>首先回答什么是BN结构？</h3><ul><li><strong>BN，即贝叶斯网络，是一种反应世界上一些事物的可能的情况的发生概率的模型，可模拟任何系统</strong></li><li><strong>BN结构，是由有向无环图(DAG)和一组条件概率表组成</strong><ul><li><strong>其中DAG每个节点表示随机变量，有向边表示一个变量对另一个变量的影响</strong></li><li><strong>条件概率表表示每个节点x在其父节点的所有可能的联合赋值条件下的x的概率分布，例如下图节点A在父节点E、B所有可能的联合赋值条件下的概率</strong></li></ul></li></ul><p><strong><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E4%B8%8E%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></strong></p><h3 id="现在可以回答BN结构学习算法了"><a href="#现在可以回答BN结构学习算法了" class="headerlink" title="现在可以回答BN结构学习算法了"></a>现在可以回答BN结构学习算法了</h3><ul><li><strong>我们使用BN的目的就是为了得到那一组条件概率表，那么首先得构建一个BN网络，即那个有向无环图</strong></li><li><strong>那么怎么构造呢，很复杂的关系没办法手动构造吧，那么就是这里讨论的BN结构学习算法了</strong></li></ul><p><strong>当然啦，那个概率表也是要训练的才行的，不过这里讨论的是BN结构哈</strong></p><ul><li><strong>该论文指出BN结构学习算法主要是三种，基于评分、基于约束、约束和评分混合</strong></li></ul><h4 id="首先是基于评分，第一步定义评分函数、第二步采用搜索策略"><a href="#首先是基于评分，第一步定义评分函数、第二步采用搜索策略" class="headerlink" title="首先是基于评分，第一步定义评分函数、第二步采用搜索策略"></a>首先是基于评分，第一步定义评分函数、第二步采用搜索策略</h4><p><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E4%B8%8E%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><ul><li><strong>该论文采用BIC评分函数，由BN结构的对数似然度和惩罚项函数组成。前者表示数据集和BN结构的拟合程度，后者防止结构模型过于复杂、参数过多，导致过拟合问题。BIC评分计算方法见如下图，这里仅作了解</strong></li></ul><p><strong><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E4%B8%8E%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/2.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></strong></p><ul><li><strong>搜索策略就是在搜索空间中找得分最高的BN结构，也就是对数据集拟合程度最高的BN结构，该论文用的GA算法，下面就会介绍</strong></li></ul><h4 id="然后是基于约束，就是计算各节点间条件是否独立，并通过构造条件独立性集合来建立BN"><a href="#然后是基于约束，就是计算各节点间条件是否独立，并通过构造条件独立性集合来建立BN" class="headerlink" title="然后是基于约束，就是计算各节点间条件是否独立，并通过构造条件独立性集合来建立BN"></a>然后是基于约束，就是计算各节点间条件是否独立，并通过构造条件独立性集合来建立BN</h4><ul><li><strong>比如对于一个节点x，我们要求他的B(x)，即与他有条件联系的节点集合，即互信息。再用CI测试给边定向</strong></li></ul><p><strong><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E4%B8%8E%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/3.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></strong></p><h4 id="最后是混合算法，即首先通过基于约束的方法构造无向图减少冗余搜索空间，然后通过基于评分搜索的方法得到评分最高的BN结构"><a href="#最后是混合算法，即首先通过基于约束的方法构造无向图减少冗余搜索空间，然后通过基于评分搜索的方法得到评分最高的BN结构" class="headerlink" title="最后是混合算法，即首先通过基于约束的方法构造无向图减少冗余搜索空间，然后通过基于评分搜索的方法得到评分最高的BN结构"></a>最后是混合算法，即首先通过基于约束的方法构造无向图减少冗余搜索空间，然后通过基于评分搜索的方法得到评分最高的BN结构</h4><hr><h2 id="第2个问题，什么是GA算法？"><a href="#第2个问题，什么是GA算法？" class="headerlink" title="第2个问题，什么是GA算法？"></a>第2个问题，什么是GA算法？</h2><h3 id="GA算法，即遗传算法。有以下几个概念："><a href="#GA算法，即遗传算法。有以下几个概念：" class="headerlink" title="GA算法，即遗传算法。有以下几个概念："></a>GA算法，即遗传算法。有以下几个概念：</h3><ul><li><strong>染色体：数学问题的每一个可行解就是一条染色体</strong></li><li><strong>基因：染色体上的元素就是基因，比如染色体[1,2,3]，那么每个数都是一个基因</strong></li><li><strong>适应度函数：每次繁衍后，适应度函数给生成的下一代所有的染色体打分，分数高的更有可能保留下去，分数低的淘汰</strong></li><li><strong>交叉：下一代染色体由父母两条染色体交叉形成，如下图所示。爸爸妈妈染色体的选择一般用轮盘赌算法，适应度高的更容易被选中</strong></li></ul><p><strong><img "" class="lazyload placeholder" data-original="/medias/%E5%88%9D%E8%AF%86%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E4%B8%8E%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/4.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></strong></p><ul><li><strong>变异：交叉每次选择都是在原有数据集上选择，容易达到局部最优，但不能全局最优。变异就是在交叉后，对染色体上的若干个基因随机修改，从而达到全局最优</strong></li></ul><h3 id="GA算法执行流程？"><a href="#GA算法执行流程？" class="headerlink" title="GA算法执行流程？"></a>GA算法执行流程？</h3><p><strong>首先设置种群大小，突变概率，交叉概率等参数，然后随机初始化个体构成初始种群。每一次通过适应度函数打分，然后交叉、变异得到下一代，不断迭代</strong></p><hr><h2 id="第3个问题，怎么使用GA算法得到BN结构？"><a href="#第3个问题，怎么使用GA算法得到BN结构？" class="headerlink" title="第3个问题，怎么使用GA算法得到BN结构？"></a>第3个问题，怎么使用GA算法得到BN结构？</h2><p><strong>将GA算法应用到BN结构学习中</strong></p><ul><li><strong>首先选择操作：将适应度函数应用到对个体的BN结构评分，评分越高说明BN结构适应度越高</strong></li><li><strong>然后交叉操作：父母BN结构进行交叉，产生下一代BN结构</strong></li><li><strong>还有变异操作：对BN结构的若干基因进行随机变异，避免达到局部最优却无法全局最优解的情况</strong></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;本笔记是对《贝叶斯网络结构学习算法研究》论文进行学习时所写&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;首先通过摘要&quot;&gt;&lt;a href=&quot;#首先通过摘要&quot; class=&quot;headerlink&quot; title=&quot;首先通过摘要&quot;&gt;&lt;/a&gt;首先通过摘要&lt;/h2&gt;&lt;p&gt;</summary>
      
    
    
    
    <category term="《贝叶斯网络结构学习算法》论文笔记" scheme="https://mactql.github.io/categories/%E3%80%8A%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="贝叶斯网络" scheme="https://mactql.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/"/>
    
    <category term="遗传算法" scheme="https://mactql.github.io/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>第1章 简单工厂模式</title>
    <link href="https://mactql.github.io/posts/4228647784.html"/>
    <id>https://mactql.github.io/posts/4228647784.html</id>
    <published>2021-08-15T07:21:00.000Z</published>
    <updated>2021-08-18T07:37:31.562Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简单工厂模式是什么？用来干啥？"><a href="#简单工厂模式是什么？用来干啥？" class="headerlink" title="简单工厂模式是什么？用来干啥？"></a>简单工厂模式是什么？用来干啥？</h2><p><strong>设想一下，如果有很多个类，都是完成类似的工作。那么就可以写一个工厂类就像一个中间人一样，来帮助我们选择需要的类来创建对象，而不用我们自己去new，UML图如下：</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC1%E7%AB%A0%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"><br><strong>比如要做一个计算器程序，需要设计加减乘除等运算类 和 去调用这些运算并显示结果的客户端类，可以创建一个工厂类</strong><br><img "" class="lazyload placeholder" data-original="/medias/%E7%AC%AC1%E7%AB%A0%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/1.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="用简单工厂模式有什么好处呢？"><a href="#用简单工厂模式有什么好处呢？" class="headerlink" title="用简单工厂模式有什么好处呢？"></a>用简单工厂模式有什么好处呢？</h2><p><strong>工厂类可以通过逻辑判断，来决定在什么时候创建哪一个产品类的实例，这样客户端可以不用自己去new对象了。简单工厂模式能够根据外界给定的信息，决定究竟应该创建哪个具体类的对象。外界与具体类隔离开来，偶合性低。明确区分了各自的职责和权力，有利于整个软件体系结构的优化</strong></p><hr><h2 id="那么怎么使用简单工厂模式呢？"><a href="#那么怎么使用简单工厂模式呢？" class="headerlink" title="那么怎么使用简单工厂模式呢？"></a>那么怎么使用简单工厂模式呢？</h2><p><strong>就上述设计计算器程序为例，抽象接口及其实现类如下</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//接口类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Operation</span></span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">getResult</span><span class="params">(<span class="keyword">double</span> numA,<span class="keyword">double</span> numB)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//实现类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OperationAdd</span> <span class="keyword">implements</span> <span class="title">Operation</span></span>&#123;</span><br><span class="line">    <span class="comment">//实现接口中的同名方法，这样客户端可以通过工厂类返回的向上转型的对象实例进行动态绑定调用具体方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">getResult</span><span class="params">(<span class="keyword">double</span> numA,<span class="keyword">double</span> numB)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> numA+numB;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//其他实现类同理</span></span><br></pre></td></tr></table></figure><p><strong>工厂类如下</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OperationFactory</span></span>&#123;</span><br><span class="line">    <span class="comment">//简单工厂类中用静态方法，返回类型为抽象接口，通过多态得到不同对象的实例</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Operation <span class="title">createOperation</span><span class="params">(String operate)</span></span>&#123;</span><br><span class="line">    <span class="keyword">switch</span>(operate)&#123;</span><br><span class="line">            <span class="keyword">if</span> operate.equals(<span class="string">&quot;+&quot;</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> OperationAdd();</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> operate.equals(<span class="string">&quot;-&quot;</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> OperationSub();</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>   ...</span><br><span class="line">                ...</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>客户端代码如下</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//通过多态的向上转型和动态绑定，虽然是抽象接口的对象，但是可以使用具体实现类的具体方法</span></span><br><span class="line">Operator oper = OperatorFactory.getOperator(<span class="string">&quot;+&quot;</span>);</span><br><span class="line"><span class="keyword">double</span> result = oper.getResult(<span class="number">1</span>,<span class="number">2</span>);</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;简单工厂模式是什么？用来干啥？&quot;&gt;&lt;a href=&quot;#简单工厂模式是什么？用来干啥？&quot; class=&quot;headerlink&quot; title=&quot;简单工厂模式是什么？用来干啥？&quot;&gt;&lt;/a&gt;简单工厂模式是什么？用来干啥？&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;设想一下，如果有很多</summary>
      
    
    
    
    <category term="《大话设计模式》读书笔记" scheme="https://mactql.github.io/categories/%E3%80%8A%E5%A4%A7%E8%AF%9D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="设计模式" scheme="https://mactql.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Maven入门指南</title>
    <link href="https://mactql.github.io/posts/477950076.html"/>
    <id>https://mactql.github.io/posts/477950076.html</id>
    <published>2021-08-14T03:35:00.000Z</published>
    <updated>2021-08-14T03:40:26.499Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么要用Maven？"><a href="#为什么要用Maven？" class="headerlink" title="为什么要用Maven？"></a>为什么要用Maven？</h2><ul><li><strong>场景一：idea开发的项目没法到eclipse中运行，即不同的ide开发的项目不能互相使用</strong></li><li><strong>场景二：大型项目需要很多个jar包，要去不同的网站下载，也不方便更新，很麻烦</strong></li></ul><hr><h2 id="IDEA上怎么创建Maven项目？"><a href="#IDEA上怎么创建Maven项目？" class="headerlink" title="IDEA上怎么创建Maven项目？"></a>IDEA上怎么创建Maven项目？</h2><p><strong>创建Maven项目时需要填写以下信息</strong></p><ul><li><strong>GroupID：机构名或者逆向域名的形式</strong></li><li><strong>ArtifactID：项目名称</strong></li><li><strong>Version：版本号</strong></li></ul><p><strong>创建好了以后会自动生成Maven项目结构以及配置文件pom.xml</strong></p><hr><h2 id="Maven项目结构是什么样的？"><a href="#Maven项目结构是什么样的？" class="headerlink" title="Maven项目结构是什么样的？"></a>Maven项目结构是什么样的？</h2><p><img "" class="lazyload placeholder" data-original="/medias/Maven%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/0.png" src="https://img10.360buyimg.com/ddimg/jfs/t1/157667/29/9156/134350/603c6445Ebbc9cabe/41219c5d36d45072.gif"></p><hr><h2 id="那pom-xml文件是干啥的？"><a href="#那pom-xml文件是干啥的？" class="headerlink" title="那pom.xml文件是干啥的？"></a>那pom.xml文件是干啥的？</h2><p><strong>pom.xml文件是用来配置项目依赖的，Maven会通过这些依赖自动下载第三方组件</strong></p><h2 id="那么怎么在pom-xml中配置项目依赖呢？"><a href="#那么怎么在pom-xml中配置项目依赖呢？" class="headerlink" title="那么怎么在pom.xml中配置项目依赖呢？"></a>那么怎么在pom.xml中配置项目依赖呢？</h2><p><strong>在search.maven.org网站中搜索需要的组件，找到需要的组件的依赖后加入pom.xml的<code>&lt;dependencies&gt;&lt;/dependencies&gt;</code>中，然后maven就会自动下载这些组件了</strong></p><hr><h2 id="怎么让maven下载依赖的速度快一点呢？"><a href="#怎么让maven下载依赖的速度快一点呢？" class="headerlink" title="怎么让maven下载依赖的速度快一点呢？"></a>怎么让maven下载依赖的速度快一点呢？</h2><p><strong>Maven首先去本地仓库找，如果本地仓库没有再去中央仓库下载到本地仓库。</strong><br><strong>为了加快下载速度，可以使用私服，从私服下载到本地仓库</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 可以在pom.xml的&lt;version&gt;下方添加一个阿里云私服的地址配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>aliyun<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://maven.aliyun.com/repository/public<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure><hr><h2 id="maven命令常用的有哪些？"><a href="#maven命令常用的有哪些？" class="headerlink" title="maven命令常用的有哪些？"></a>maven命令常用的有哪些？</h2><ul><li><strong>compile：编译，编译成功后会增加一个target目录</strong></li><li><strong>clean：删除整个target目录</strong></li><li><strong>test：在集成junit，并且有test项目和测试类</strong></li><li><strong>package：把项目打包成jar包</strong></li><li><strong>install：把打好的jar包放到本地仓库去</strong></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;为什么要用Maven？&quot;&gt;&lt;a href=&quot;#为什么要用Maven？&quot; class=&quot;headerlink&quot; title=&quot;为什么要用Maven？&quot;&gt;&lt;/a&gt;为什么要用Maven？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;场景一：idea开发的项目没法到ecli</summary>
      
    
    
    
    <category term="《Maven项目管理》课程笔记" scheme="https://mactql.github.io/categories/%E3%80%8AMaven%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Maven" scheme="https://mactql.github.io/tags/Maven/"/>
    
  </entry>
  
</feed>
